{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:12:56.247374Z",
     "start_time": "2024-10-21T19:12:56.234067Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV"
   ],
   "outputs": [],
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:12:56.726151Z",
     "start_time": "2024-10-21T19:12:56.646811Z"
    }
   },
   "source": [
    "file_path = 'Data assignment 1/Feature data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data2=pd.read_csv(\"Data process/Forcasted Weather data new.csv\")"
   ],
   "outputs": [],
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:12:57.044756Z",
     "start_time": "2024-10-21T19:12:57.020877Z"
    }
   },
   "source": [
    "#Dropping the features that are not required for the analysis\n",
    "data2=data2.drop('Accumulated percipitation', axis=1)\n",
    "data2=data2.drop('Mean humidity', axis=1)\n",
    "data2=data2.drop('Wind Speed Y Direction', axis=1)\n",
    "data2=data2.drop('Wind Speed X Direction', axis=1)\n",
    "data2=data2.drop('Mean temperature', axis=1)\n",
    "data2=data2.drop('Minimum temperature', axis=1)\n",
    "data2=data2.drop('Solar Shortwave Flux', axis=1)\n",
    "data2=data2.drop('Unnamed: 0',axis=1)\n",
    "data2.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    ts  Maximum temperature  Mean wind direction  \\\n",
       "0  2022-01-01 00:00:00            279.84314            235.57857   \n",
       "1  2022-01-01 01:00:00            279.84314            235.57857   \n",
       "2  2022-01-01 02:00:00            279.84314            235.57857   \n",
       "3  2022-01-01 03:00:00            279.84314            235.57857   \n",
       "4  2022-01-01 04:00:00            279.84314            235.57857   \n",
       "\n",
       "   Mean wind speed  \n",
       "0         5.813893  \n",
       "1         5.813893  \n",
       "2         5.813893  \n",
       "3         5.813893  \n",
       "4         5.813893  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>Maximum temperature</th>\n",
       "      <th>Mean wind direction</th>\n",
       "      <th>Mean wind speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>279.84314</td>\n",
       "      <td>235.57857</td>\n",
       "      <td>5.813893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-01 01:00:00</td>\n",
       "      <td>279.84314</td>\n",
       "      <td>235.57857</td>\n",
       "      <td>5.813893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-01 02:00:00</td>\n",
       "      <td>279.84314</td>\n",
       "      <td>235.57857</td>\n",
       "      <td>5.813893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-01 03:00:00</td>\n",
       "      <td>279.84314</td>\n",
       "      <td>235.57857</td>\n",
       "      <td>5.813893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01 04:00:00</td>\n",
       "      <td>279.84314</td>\n",
       "      <td>235.57857</td>\n",
       "      <td>5.813893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step the different features are scaled, in order to make sure the models can interpret the features on a similar scale. The wind speed and the maximum temperature undergo the standardscaler, while wind direction is converted to sinus and cosinus components. The power production is normalized using the nominal capacity of 30 MW (https://stateofgreen.com/en/solutions/kalby-wind-turbines/). \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:12:58.909403Z",
     "start_time": "2024-10-21T19:12:58.883143Z"
    }
   },
   "source": [
    "# Import required scalers\n",
    "scaler_standard = StandardScaler()\n",
    "\n",
    "### 1. Standard Scaling for wind speed and temperature\n",
    "data['Mean wind speed'] = scaler_standard.fit_transform(data[['Mean wind speed']])\n",
    "data['Maximum temperature'] = scaler_standard.fit_transform(data[['Maximum temperature']])\n",
    "\n",
    "### 2. Wind Direction (convert to sin and cos components)\n",
    "data['Wind direction sin'] = np.sin(np.deg2rad(data['Mean wind direction']))\n",
    "data['Wind direction cos'] = np.cos(np.deg2rad(data['Mean wind direction']))\n",
    "\n",
    "### 3. Normalize Power Production \n",
    "nominal_capacity = 30000 # production capacity is 30 MW, unit of power production is kW so nominal capacity is 30000 (kW)\n",
    "data['AKI Kalby Active Power'] = data['AKI Kalby Active Power'] / nominal_capacity\n",
    "\n",
    "# Dropping the original wind direction after scaling\n",
    "data = data.drop('Mean wind direction', axis=1)"
   ],
   "outputs": [],
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:12:59.574886Z",
     "start_time": "2024-10-21T19:12:59.550135Z"
    }
   },
   "source": [
    "### 1. Standard Scaling for wind speed and temperature for the Forecasted Weather Data\n",
    "data2['Mean wind speed'] = scaler_standard.fit_transform(data2[['Mean wind speed']])\n",
    "data2['Maximum temperature'] = scaler_standard.fit_transform(data2[['Maximum temperature']])\n",
    "\n",
    "### 2. Wind Direction (convert to sin and cos components) for the Forecasted Weather Data\n",
    "data2['Wind direction sin'] = np.sin(np.deg2rad(data2['Mean wind direction']))\n",
    "data2['Wind direction cos'] = np.cos(np.deg2rad(data2['Mean wind direction']))\n",
    "\n",
    "\n",
    "# Dropping the original wind direction after transformation for the Forecasted Weather Data\n",
    "data2 = data2.drop('Mean wind direction', axis=1)"
   ],
   "outputs": [],
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:12:59.946333Z",
     "start_time": "2024-10-21T19:12:59.924522Z"
    }
   },
   "source": [
    "# Make sure datetime is set as the index\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "data.set_index('datetime', inplace=True)"
   ],
   "outputs": [],
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:13:00.299406Z",
     "start_time": "2024-10-21T19:13:00.280541Z"
    }
   },
   "source": [
    "# datetime is set as the index for the Forecasted Weather Data\n",
    "data2['ts'] = pd.to_datetime(data2['ts'])\n",
    "data2.set_index('ts', inplace=True)"
   ],
   "outputs": [],
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:13:00.790194Z",
     "start_time": "2024-10-21T19:13:00.778424Z"
    }
   },
   "source": [
    "# Set target and features, and remove non-numeric columns\n",
    "target_column = 'AKI Kalby Active Power'\n",
    "features = data.select_dtypes(include=[np.number]).drop(columns=[target_column])"
   ],
   "outputs": [],
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:13:01.423899Z",
     "start_time": "2024-10-21T19:13:01.409626Z"
    }
   },
   "source": [
    "# Load the dataframe to check if scaling worked\n",
    "data.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                     AKI Kalby Active Power  Maximum temperature  \\\n",
       "datetime                                                           \n",
       "2022-01-01 00:00:00               -0.063118            -0.457945   \n",
       "2022-01-01 01:00:00               -0.055728            -0.457945   \n",
       "2022-01-01 02:00:00               -0.095724            -0.503187   \n",
       "2022-01-01 03:00:00               -0.063726            -0.518268   \n",
       "2022-01-01 04:00:00               -0.029392            -0.473025   \n",
       "\n",
       "                     Mean wind speed  Wind direction sin  Wind direction cos  \n",
       "datetime                                                                      \n",
       "2022-01-01 00:00:00         0.868655           -0.998630       -5.233596e-02  \n",
       "2022-01-01 01:00:00         0.382418           -0.956305       -2.923717e-01  \n",
       "2022-01-01 02:00:00         0.756447           -0.994522       -1.045285e-01  \n",
       "2022-01-01 03:00:00         0.494627           -1.000000       -1.836970e-16  \n",
       "2022-01-01 04:00:00         0.307612           -0.951057        3.090170e-01  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AKI Kalby Active Power</th>\n",
       "      <th>Maximum temperature</th>\n",
       "      <th>Mean wind speed</th>\n",
       "      <th>Wind direction sin</th>\n",
       "      <th>Wind direction cos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:00:00</th>\n",
       "      <td>-0.063118</td>\n",
       "      <td>-0.457945</td>\n",
       "      <td>0.868655</td>\n",
       "      <td>-0.998630</td>\n",
       "      <td>-5.233596e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 01:00:00</th>\n",
       "      <td>-0.055728</td>\n",
       "      <td>-0.457945</td>\n",
       "      <td>0.382418</td>\n",
       "      <td>-0.956305</td>\n",
       "      <td>-2.923717e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 02:00:00</th>\n",
       "      <td>-0.095724</td>\n",
       "      <td>-0.503187</td>\n",
       "      <td>0.756447</td>\n",
       "      <td>-0.994522</td>\n",
       "      <td>-1.045285e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 03:00:00</th>\n",
       "      <td>-0.063726</td>\n",
       "      <td>-0.518268</td>\n",
       "      <td>0.494627</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 04:00:00</th>\n",
       "      <td>-0.029392</td>\n",
       "      <td>-0.473025</td>\n",
       "      <td>0.307612</td>\n",
       "      <td>-0.951057</td>\n",
       "      <td>3.090170e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 90
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing testing and training \n",
    "Using train_test_split the data is split into testing and training data. The choice was made to not use TimeSeriesSplit, because although the data is time based, the values are not dependent on the time of day in the sense that there is no strong temporal relationship that affects the observations. The power production is more weather dependent than anything else. \n",
    " The data points can be treated independently of their time indices, allowing for a standard random sampling approach. By maintaining a randomized split, we also prevent potential biases that could arise from time-based sequences, ensuring that both the training and testing set represent the overall distribution of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:58:26.293742Z",
     "start_time": "2024-10-21T19:58:26.271584Z"
    }
   },
   "source": [
    "# Split the data\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "# Select a 100 datapoints as a start with a low number of samples\n",
    "X_sample = features[:100]\n",
    "y_sample = data[target_column][:100]\n",
    "\n",
    "# Sequential split (shuffle=False)\n",
    "# Give the random_state a set seed of 42, to ensure that the split will be the same everytime in order to reproduce results\n",
    "X_sample_train, X_sample_test, y_sample_train, y_sample_test = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.2, shuffle=False, random_state=42)\n",
    "\n",
    "#Ensure that they are the same data points, for easy comparison\n",
    "X_forecast_sample = data2[data2.index.isin(pd.to_datetime(y_sample_test.index))]\n",
    "X_forecast_sample = X_forecast_sample.reindex(y_sample_test.index)\n",
    "\n",
    "# Adding a column of ones to X_sample for the bias term and converting to NumPy array\n",
    "X_sample_train_with_bias = np.c_[np.ones(X_sample_train.shape[0]), X_sample_train].astype(float)\n",
    "X_sample_test_with_bias = np.c_[np.ones(X_sample_test.shape[0]), X_sample_test].astype(float)\n",
    "X_forecast_sample_with_bias=np.c_[np.ones(X_forecast_sample.shape[0]), X_forecast_sample].astype(float)\n",
    "\n",
    "# Ensure y_sample_train is also a NumPy array\n",
    "y_sample_train = np.array(y_sample_train).astype(float)"
   ],
   "outputs": [],
   "execution_count": 202
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:58:29.254275Z",
     "start_time": "2024-10-21T19:58:29.248244Z"
    }
   },
   "source": [
    "data2_with_bias=np.c_[np.ones(data2.shape[0]), data2].astype(float)"
   ],
   "outputs": [],
   "execution_count": 203
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:58:33.621401Z",
     "start_time": "2024-10-21T19:58:32.039551Z"
    }
   },
   "source": [
    "# Gradient Descent function\n",
    "def gradient_descent(X, y, learning_rate=0.01, epochs=100000):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    for _ in range(epochs):\n",
    "        y_pred = X @ theta\n",
    "        gradients = (1/m) * X.T @ (y_pred - y)\n",
    "        theta -= learning_rate * gradients\n",
    "    return theta\n",
    "\n",
    "theta_gd = gradient_descent(X_sample_train_with_bias, y_sample_train)\n",
    "\n",
    "# Predictions using Gradient Descent\n",
    "#y_pred_gd = X_sample_test_with_bias @ theta_gd"
   ],
   "outputs": [],
   "execution_count": 204
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:00:46.889684Z",
     "start_time": "2024-10-21T20:00:46.879867Z"
    }
   },
   "source": [
    "# Predictions for Forecasted Data using Gradient Descent\n",
    "y_pred_gd = X_sample_test_with_bias @ theta_gd\n",
    "y_pred_gd_forecast=X_forecast_sample_with_bias @ theta_gd"
   ],
   "outputs": [],
   "execution_count": 209
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:00:52.941577Z",
     "start_time": "2024-10-21T20:00:52.923218Z"
    }
   },
   "source": [
    "# Closed-form solution \n",
    "theta_closed_form = np.linalg.inv(X_sample_train_with_bias.T @ X_sample_train_with_bias) @ X_sample_train_with_bias.T @ y_sample_train\n",
    "\n",
    "# Predictions using closed-form solution\n",
    "y_pred_closed_form = X_forecast_sample_with_bias @ theta_closed_form"
   ],
   "outputs": [],
   "execution_count": 210
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:01:10.769192Z",
     "start_time": "2024-10-21T20:01:10.758273Z"
    }
   },
   "source": [
    "# mse calculation\n",
    "mse_gd = mean_squared_error(y_sample_test, y_pred_gd_forecast)\n",
    "mse_closed_form = mean_squared_error(y_sample_test, y_pred_closed_form)\n",
    "\n",
    "print(f\"Gradient Descent θ: {[f'{x:.5f}' for x in theta_gd]}\")\n",
    "print(f\"Closed-Form θ: {[f'{x:.5f}' for x in theta_closed_form]}\")\n",
    "print(f\"Gradient Descent MSE: {mse_gd:.5f}\")\n",
    "print(f\"Closed-Form MSE: {mse_closed_form:.5f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent θ: ['-0.03789', '0.02592', '-0.04119', '0.01152', '0.01116']\n",
      "Closed-Form θ: ['-0.03789', '0.02592', '-0.04119', '0.01152', '0.01116']\n",
      "Gradient Descent MSE: 0.00076\n",
      "Closed-Form MSE: 0.00076\n"
     ]
    }
   ],
   "execution_count": 212
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:02:10.420603Z",
     "start_time": "2024-10-21T20:02:10.392985Z"
    }
   },
   "source": [
    "# Step 3.2: Use the full dataset and closed form solution\n",
    "X_large_sample, X_large_test_sample, y_large_sample, y_large_test_sample = train_test_split(features, data[target_column], test_size=0.2, random_state=42)\n",
    "X_large_sample_forecast = data2[data2.index.isin(pd.to_datetime(y_large_test_sample.index))]\n",
    "X_large_sample_forecast = X_large_sample_forecast.reindex(y_large_test_sample.index)\n",
    "                              \n",
    "# Adding a column of ones for the bias term in the large sample\n",
    "X_large_sample_with_bias = np.c_[np.ones(X_large_sample.shape[0]), X_large_sample]\n",
    "X_large_test_sample_with_bias = np.c_[np.ones(X_large_test_sample.shape[0]), X_large_test_sample]\n",
    "X_large_sample_forecast_with_bias=np.c_[np.ones(X_large_sample_forecast.shape[0]), X_large_sample_forecast]\n",
    "\n",
    "# Upgrade the normal equation\n",
    "theta_large_sample = np.linalg.inv(X_large_sample_with_bias.T @ X_large_sample_with_bias) @ X_large_sample_with_bias.T @ y_large_sample\n",
    "theta_large_sample_rounded = np.round(theta_large_sample, 5)\n",
    "\n",
    "print(f\"Step 3.2: Closed-form solution training complete on the larger sample.\")\n",
    "print(f\"Coefficients: {[f'{x:.5f}' for x in theta_large_sample_rounded]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3.2: Closed-form solution training complete on the larger sample.\n",
      "Coefficients: ['-0.04394', '0.00016', '-0.03909', '0.00591', '0.00366']\n"
     ]
    }
   ],
   "execution_count": 213
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:50:42.937927Z",
     "start_time": "2024-10-21T19:50:42.927556Z"
    }
   },
   "source": [
    "# #Resetting the index, to obtain the timestamp as a seperate column and renaming it to Datetime\n",
    "# X_large_test_sample.reset_index(inplace=True)\n",
    "# data2_reset=data2.reset_index()\n",
    "# data2_reset['datetime']=data2_reset['ts']"
   ],
   "outputs": [],
   "execution_count": 182
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:30:20.725865Z",
     "start_time": "2024-10-21T19:30:20.691872Z"
    }
   },
   "source": [
    "# #Merging the old dataset (Weather data) and the new dataset (Forecasted Data)\n",
    "# X_large_test_sample = X_large_test_sample.set_index('datetime')\n",
    "# # data2_reset = data2_reset.set_index('datetime')\n",
    "# merged_ds = X_large_test_sample.join(data2_reset, how='left', lsuffix='_left', rsuffix='').reset_index()\n",
    "# merged_ds"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                datetime  Maximum temperature_left  Mean wind speed_left  \\\n",
       "0    2022-05-20 06:00:00                  1.427152              0.569432   \n",
       "1    2022-11-15 09:00:00                 -0.156329             -0.515250   \n",
       "2    2022-05-20 01:00:00                  1.819252              0.457224   \n",
       "3    2022-12-22 23:00:00                 -0.653995              0.906058   \n",
       "4    2022-08-20 20:00:00                  1.442233             -1.038890   \n",
       "...                  ...                       ...                   ...   \n",
       "1558 2022-09-03 12:00:00                  1.246183              0.307612   \n",
       "1559 2022-09-12 16:00:00                  1.065214             -0.552653   \n",
       "1560 2022-05-05 23:00:00                 -0.473025             -0.477847   \n",
       "1561 2022-04-01 16:00:00                 -1.106418              1.654115   \n",
       "1562 2022-03-30 03:00:00                 -1.332629              0.083195   \n",
       "\n",
       "      Wind direction sin_left  Wind direction cos_left                  ts  \\\n",
       "0                   -0.909961                -0.414693 2022-05-20 06:00:00   \n",
       "1                    0.898794                -0.438371 2022-11-15 09:00:00   \n",
       "2                   -0.544639                -0.838671 2022-05-20 01:00:00   \n",
       "3                   -0.891007                -0.453990 2022-12-22 23:00:00   \n",
       "4                   -0.913545                 0.406737 2022-08-20 20:00:00   \n",
       "...                       ...                      ...                 ...   \n",
       "1558                 0.961262                -0.275637 2022-09-03 12:00:00   \n",
       "1559                -0.707107                -0.707107 2022-09-12 16:00:00   \n",
       "1560                -0.974370                 0.224951 2022-05-05 23:00:00   \n",
       "1561                 0.681998                 0.731354 2022-04-01 16:00:00   \n",
       "1562                 0.819152                 0.573576 2022-03-30 03:00:00   \n",
       "\n",
       "      Maximum temperature  Mean wind speed  Wind direction sin  \\\n",
       "0                0.963624        -0.130353           -0.890382   \n",
       "1               -0.258735        -0.626248            0.964713   \n",
       "2                1.027204        -0.659087           -0.994775   \n",
       "3               -0.744850         1.032841           -0.929767   \n",
       "4                1.300281        -0.941475           -0.572338   \n",
       "...                   ...              ...                 ...   \n",
       "1558             1.147728        -0.344476            0.987257   \n",
       "1559             0.913578        -0.812598            0.249059   \n",
       "1560            -0.339004        -1.200294           -0.866022   \n",
       "1561            -1.095397         1.063045            0.743966   \n",
       "1562            -1.361007        -0.371022            0.917115   \n",
       "\n",
       "      Wind direction cos  \n",
       "0               0.455215  \n",
       "1              -0.263303  \n",
       "2               0.102093  \n",
       "3              -0.368149  \n",
       "4               0.820018  \n",
       "...                  ...  \n",
       "1558           -0.159132  \n",
       "1559           -0.968488  \n",
       "1560            0.500006  \n",
       "1561            0.668218  \n",
       "1562            0.398623  \n",
       "\n",
       "[1563 rows x 10 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>Maximum temperature_left</th>\n",
       "      <th>Mean wind speed_left</th>\n",
       "      <th>Wind direction sin_left</th>\n",
       "      <th>Wind direction cos_left</th>\n",
       "      <th>ts</th>\n",
       "      <th>Maximum temperature</th>\n",
       "      <th>Mean wind speed</th>\n",
       "      <th>Wind direction sin</th>\n",
       "      <th>Wind direction cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-20 06:00:00</td>\n",
       "      <td>1.427152</td>\n",
       "      <td>0.569432</td>\n",
       "      <td>-0.909961</td>\n",
       "      <td>-0.414693</td>\n",
       "      <td>2022-05-20 06:00:00</td>\n",
       "      <td>0.963624</td>\n",
       "      <td>-0.130353</td>\n",
       "      <td>-0.890382</td>\n",
       "      <td>0.455215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-15 09:00:00</td>\n",
       "      <td>-0.156329</td>\n",
       "      <td>-0.515250</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>2022-11-15 09:00:00</td>\n",
       "      <td>-0.258735</td>\n",
       "      <td>-0.626248</td>\n",
       "      <td>0.964713</td>\n",
       "      <td>-0.263303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-05-20 01:00:00</td>\n",
       "      <td>1.819252</td>\n",
       "      <td>0.457224</td>\n",
       "      <td>-0.544639</td>\n",
       "      <td>-0.838671</td>\n",
       "      <td>2022-05-20 01:00:00</td>\n",
       "      <td>1.027204</td>\n",
       "      <td>-0.659087</td>\n",
       "      <td>-0.994775</td>\n",
       "      <td>0.102093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-22 23:00:00</td>\n",
       "      <td>-0.653995</td>\n",
       "      <td>0.906058</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453990</td>\n",
       "      <td>2022-12-22 23:00:00</td>\n",
       "      <td>-0.744850</td>\n",
       "      <td>1.032841</td>\n",
       "      <td>-0.929767</td>\n",
       "      <td>-0.368149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-08-20 20:00:00</td>\n",
       "      <td>1.442233</td>\n",
       "      <td>-1.038890</td>\n",
       "      <td>-0.913545</td>\n",
       "      <td>0.406737</td>\n",
       "      <td>2022-08-20 20:00:00</td>\n",
       "      <td>1.300281</td>\n",
       "      <td>-0.941475</td>\n",
       "      <td>-0.572338</td>\n",
       "      <td>0.820018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>2022-09-03 12:00:00</td>\n",
       "      <td>1.246183</td>\n",
       "      <td>0.307612</td>\n",
       "      <td>0.961262</td>\n",
       "      <td>-0.275637</td>\n",
       "      <td>2022-09-03 12:00:00</td>\n",
       "      <td>1.147728</td>\n",
       "      <td>-0.344476</td>\n",
       "      <td>0.987257</td>\n",
       "      <td>-0.159132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>2022-09-12 16:00:00</td>\n",
       "      <td>1.065214</td>\n",
       "      <td>-0.552653</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>2022-09-12 16:00:00</td>\n",
       "      <td>0.913578</td>\n",
       "      <td>-0.812598</td>\n",
       "      <td>0.249059</td>\n",
       "      <td>-0.968488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <td>2022-05-05 23:00:00</td>\n",
       "      <td>-0.473025</td>\n",
       "      <td>-0.477847</td>\n",
       "      <td>-0.974370</td>\n",
       "      <td>0.224951</td>\n",
       "      <td>2022-05-05 23:00:00</td>\n",
       "      <td>-0.339004</td>\n",
       "      <td>-1.200294</td>\n",
       "      <td>-0.866022</td>\n",
       "      <td>0.500006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>2022-04-01 16:00:00</td>\n",
       "      <td>-1.106418</td>\n",
       "      <td>1.654115</td>\n",
       "      <td>0.681998</td>\n",
       "      <td>0.731354</td>\n",
       "      <td>2022-04-01 16:00:00</td>\n",
       "      <td>-1.095397</td>\n",
       "      <td>1.063045</td>\n",
       "      <td>0.743966</td>\n",
       "      <td>0.668218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>2022-03-30 03:00:00</td>\n",
       "      <td>-1.332629</td>\n",
       "      <td>0.083195</td>\n",
       "      <td>0.819152</td>\n",
       "      <td>0.573576</td>\n",
       "      <td>2022-03-30 03:00:00</td>\n",
       "      <td>-1.361007</td>\n",
       "      <td>-0.371022</td>\n",
       "      <td>0.917115</td>\n",
       "      <td>0.398623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1563 rows × 10 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 146
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:35:13.223510Z",
     "start_time": "2024-10-21T19:35:08.728482Z"
    }
   },
   "source": [
    "# #Dropping the columns from the old dataset\n",
    "# merged_ds=merged_ds.drop('Maximum temperature_left', axis=1)\n",
    "# merged_ds=merged_ds.drop('Mean wind speed_left', axis=1)\n",
    "# merged_ds=merged_ds.drop('Wind direction sin_left', axis=1)\n",
    "# merged_ds=merged_ds.drop('Wind direction cos_left', axis=1)\n",
    "# merged_ds=merged_ds.drop('ts', axis=1)\n",
    "# merged_ds.head(1)"
   ],
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Maximum temperature_left'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[161], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#Dropping the columns from the old dataset\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m merged_ds\u001B[38;5;241m=\u001B[39mmerged_ds\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMaximum temperature_left\u001B[39m\u001B[38;5;124m'\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      3\u001B[0m merged_ds\u001B[38;5;241m=\u001B[39mmerged_ds\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMean wind speed_left\u001B[39m\u001B[38;5;124m'\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      4\u001B[0m merged_ds\u001B[38;5;241m=\u001B[39mmerged_ds\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWind direction sin_left\u001B[39m\u001B[38;5;124m'\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5344\u001B[0m, in \u001B[0;36mDataFrame.drop\u001B[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[0;32m   5196\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdrop\u001B[39m(\n\u001B[0;32m   5197\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   5198\u001B[0m     labels: IndexLabel \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   5205\u001B[0m     errors: IgnoreRaise \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   5206\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   5207\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   5208\u001B[0m \u001B[38;5;124;03m    Drop specified labels from rows or columns.\u001B[39;00m\n\u001B[0;32m   5209\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   5342\u001B[0m \u001B[38;5;124;03m            weight  1.0     0.8\u001B[39;00m\n\u001B[0;32m   5343\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 5344\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mdrop(\n\u001B[0;32m   5345\u001B[0m         labels\u001B[38;5;241m=\u001B[39mlabels,\n\u001B[0;32m   5346\u001B[0m         axis\u001B[38;5;241m=\u001B[39maxis,\n\u001B[0;32m   5347\u001B[0m         index\u001B[38;5;241m=\u001B[39mindex,\n\u001B[0;32m   5348\u001B[0m         columns\u001B[38;5;241m=\u001B[39mcolumns,\n\u001B[0;32m   5349\u001B[0m         level\u001B[38;5;241m=\u001B[39mlevel,\n\u001B[0;32m   5350\u001B[0m         inplace\u001B[38;5;241m=\u001B[39minplace,\n\u001B[0;32m   5351\u001B[0m         errors\u001B[38;5;241m=\u001B[39merrors,\n\u001B[0;32m   5352\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4711\u001B[0m, in \u001B[0;36mNDFrame.drop\u001B[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[0;32m   4709\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m axis, labels \u001B[38;5;129;01min\u001B[39;00m axes\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m   4710\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 4711\u001B[0m         obj \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39m_drop_axis(labels, axis, level\u001B[38;5;241m=\u001B[39mlevel, errors\u001B[38;5;241m=\u001B[39merrors)\n\u001B[0;32m   4713\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inplace:\n\u001B[0;32m   4714\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_inplace(obj)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4753\u001B[0m, in \u001B[0;36mNDFrame._drop_axis\u001B[1;34m(self, labels, axis, level, errors, only_slice)\u001B[0m\n\u001B[0;32m   4751\u001B[0m         new_axis \u001B[38;5;241m=\u001B[39m axis\u001B[38;5;241m.\u001B[39mdrop(labels, level\u001B[38;5;241m=\u001B[39mlevel, errors\u001B[38;5;241m=\u001B[39merrors)\n\u001B[0;32m   4752\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 4753\u001B[0m         new_axis \u001B[38;5;241m=\u001B[39m axis\u001B[38;5;241m.\u001B[39mdrop(labels, errors\u001B[38;5;241m=\u001B[39merrors)\n\u001B[0;32m   4754\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m axis\u001B[38;5;241m.\u001B[39mget_indexer(new_axis)\n\u001B[0;32m   4756\u001B[0m \u001B[38;5;66;03m# Case for non-unique axis\u001B[39;00m\n\u001B[0;32m   4757\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7000\u001B[0m, in \u001B[0;36mIndex.drop\u001B[1;34m(self, labels, errors)\u001B[0m\n\u001B[0;32m   6998\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask\u001B[38;5;241m.\u001B[39many():\n\u001B[0;32m   6999\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m errors \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m-> 7000\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlabels[mask]\u001B[38;5;241m.\u001B[39mtolist()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found in axis\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   7001\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m indexer[\u001B[38;5;241m~\u001B[39mmask]\n\u001B[0;32m   7002\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdelete(indexer)\n",
      "\u001B[1;31mKeyError\u001B[0m: \"['Maximum temperature_left'] not found in axis\""
     ]
    }
   ],
   "execution_count": 161
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:35:18.305059Z",
     "start_time": "2024-10-21T19:35:18.284120Z"
    }
   },
   "cell_type": "code",
   "source": "# merged_ds",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                     Maximum temperature  Mean wind speed  Wind direction sin  \\\n",
       "datetime                                                                        \n",
       "2022-05-20 06:00:00             0.963624        -0.130353           -0.890382   \n",
       "2022-11-15 09:00:00            -0.258735        -0.626248            0.964713   \n",
       "2022-05-20 01:00:00             1.027204        -0.659087           -0.994775   \n",
       "2022-12-22 23:00:00            -0.744850         1.032841           -0.929767   \n",
       "2022-08-20 20:00:00             1.300281        -0.941475           -0.572338   \n",
       "...                                  ...              ...                 ...   \n",
       "2022-09-03 12:00:00             1.147728        -0.344476            0.987257   \n",
       "2022-09-12 16:00:00             0.913578        -0.812598            0.249059   \n",
       "2022-05-05 23:00:00            -0.339004        -1.200294           -0.866022   \n",
       "2022-04-01 16:00:00            -1.095397         1.063045            0.743966   \n",
       "2022-03-30 03:00:00            -1.361007        -0.371022            0.917115   \n",
       "\n",
       "                     Wind direction cos  \n",
       "datetime                                 \n",
       "2022-05-20 06:00:00            0.455215  \n",
       "2022-11-15 09:00:00           -0.263303  \n",
       "2022-05-20 01:00:00            0.102093  \n",
       "2022-12-22 23:00:00           -0.368149  \n",
       "2022-08-20 20:00:00            0.820018  \n",
       "...                                 ...  \n",
       "2022-09-03 12:00:00           -0.159132  \n",
       "2022-09-12 16:00:00           -0.968488  \n",
       "2022-05-05 23:00:00            0.500006  \n",
       "2022-04-01 16:00:00            0.668218  \n",
       "2022-03-30 03:00:00            0.398623  \n",
       "\n",
       "[1563 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Maximum temperature</th>\n",
       "      <th>Mean wind speed</th>\n",
       "      <th>Wind direction sin</th>\n",
       "      <th>Wind direction cos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-05-20 06:00:00</th>\n",
       "      <td>0.963624</td>\n",
       "      <td>-0.130353</td>\n",
       "      <td>-0.890382</td>\n",
       "      <td>0.455215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-15 09:00:00</th>\n",
       "      <td>-0.258735</td>\n",
       "      <td>-0.626248</td>\n",
       "      <td>0.964713</td>\n",
       "      <td>-0.263303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-20 01:00:00</th>\n",
       "      <td>1.027204</td>\n",
       "      <td>-0.659087</td>\n",
       "      <td>-0.994775</td>\n",
       "      <td>0.102093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-22 23:00:00</th>\n",
       "      <td>-0.744850</td>\n",
       "      <td>1.032841</td>\n",
       "      <td>-0.929767</td>\n",
       "      <td>-0.368149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-20 20:00:00</th>\n",
       "      <td>1.300281</td>\n",
       "      <td>-0.941475</td>\n",
       "      <td>-0.572338</td>\n",
       "      <td>0.820018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-03 12:00:00</th>\n",
       "      <td>1.147728</td>\n",
       "      <td>-0.344476</td>\n",
       "      <td>0.987257</td>\n",
       "      <td>-0.159132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-12 16:00:00</th>\n",
       "      <td>0.913578</td>\n",
       "      <td>-0.812598</td>\n",
       "      <td>0.249059</td>\n",
       "      <td>-0.968488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-05 23:00:00</th>\n",
       "      <td>-0.339004</td>\n",
       "      <td>-1.200294</td>\n",
       "      <td>-0.866022</td>\n",
       "      <td>0.500006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-01 16:00:00</th>\n",
       "      <td>-1.095397</td>\n",
       "      <td>1.063045</td>\n",
       "      <td>0.743966</td>\n",
       "      <td>0.668218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-30 03:00:00</th>\n",
       "      <td>-1.361007</td>\n",
       "      <td>-0.371022</td>\n",
       "      <td>0.917115</td>\n",
       "      <td>0.398623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1563 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 162
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:30:25.383209Z",
     "start_time": "2024-10-21T19:30:25.375890Z"
    }
   },
   "source": "# merged_ds_with_bias=np.c_[np.ones(merged_ds.shape[0]), merged_ds]",
   "outputs": [],
   "execution_count": 148
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:03:26.991033Z",
     "start_time": "2024-10-21T20:03:26.894937Z"
    }
   },
   "source": [
    "# # Step 3.3: Verify your model using the testing dataset and appropriate evaluation metrics\n",
    "# y_large_pred_closed_form = X_large_sample_with_bias @ theta_large_sample\n",
    "# \n",
    "# mse = mean_squared_error(y_large_test_sample, y_large_pred_closed_form)\n",
    "# mae = mean_absolute_error(y_large_test_sample, y_large_pred_closed_form)\n",
    "# r2 = r2_score(y_large_test_sample, y_large_pred_closed_form)\n",
    "# rmse = np.sqrt(mse)\n",
    "# \n",
    "# print(f\"Step 3.3: Model evaluation on the testing dataset:\")\n",
    "# print(f\"Root Mean Squared Error (RMSE): {rmse:.5f}\")\n",
    "# print(f\"Mean Squared Error (MSE): {mse:.5f}\")\n",
    "# print(f\"Mean Absolute Error (MAE): {mae:.5f}\")\n",
    "# print(f\"R-squared: {r2:.5f}\")"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1563, 6250]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[217], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Step 3.3: Verify your model using the testing dataset and appropriate evaluation metrics\u001B[39;00m\n\u001B[0;32m      2\u001B[0m y_large_pred_closed_form \u001B[38;5;241m=\u001B[39m X_large_sample_with_bias \u001B[38;5;241m@\u001B[39m theta_large_sample\n\u001B[1;32m----> 4\u001B[0m mse \u001B[38;5;241m=\u001B[39m mean_squared_error(y_large_test_sample, y_large_pred_closed_form)\n\u001B[0;32m      5\u001B[0m mae \u001B[38;5;241m=\u001B[39m mean_absolute_error(y_large_test_sample, y_large_pred_closed_form)\n\u001B[0;32m      6\u001B[0m r2 \u001B[38;5;241m=\u001B[39m r2_score(y_large_test_sample, y_large_pred_closed_form)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:442\u001B[0m, in \u001B[0;36mmean_squared_error\u001B[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001B[0m\n\u001B[0;32m    382\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmean_squared_error\u001B[39m(\n\u001B[0;32m    383\u001B[0m     y_true, y_pred, \u001B[38;5;241m*\u001B[39m, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, multioutput\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muniform_average\u001B[39m\u001B[38;5;124m\"\u001B[39m, squared\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    384\u001B[0m ):\n\u001B[0;32m    385\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Mean squared error regression loss.\u001B[39;00m\n\u001B[0;32m    386\u001B[0m \n\u001B[0;32m    387\u001B[0m \u001B[38;5;124;03m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    440\u001B[0m \u001B[38;5;124;03m    0.825...\u001B[39;00m\n\u001B[0;32m    441\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 442\u001B[0m     y_type, y_true, y_pred, multioutput \u001B[38;5;241m=\u001B[39m _check_reg_targets(\n\u001B[0;32m    443\u001B[0m         y_true, y_pred, multioutput\n\u001B[0;32m    444\u001B[0m     )\n\u001B[0;32m    445\u001B[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001B[0;32m    446\u001B[0m     output_errors \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39maverage((y_true \u001B[38;5;241m-\u001B[39m y_pred) \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, weights\u001B[38;5;241m=\u001B[39msample_weight)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:100\u001B[0m, in \u001B[0;36m_check_reg_targets\u001B[1;34m(y_true, y_pred, multioutput, dtype)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_reg_targets\u001B[39m(y_true, y_pred, multioutput, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumeric\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m     67\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001B[39;00m\n\u001B[0;32m     68\u001B[0m \n\u001B[0;32m     69\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;124;03m        correct keyword.\u001B[39;00m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 100\u001B[0m     check_consistent_length(y_true, y_pred)\n\u001B[0;32m    101\u001B[0m     y_true \u001B[38;5;241m=\u001B[39m check_array(y_true, ensure_2d\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[0;32m    102\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m check_array(y_pred, ensure_2d\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39mdtype)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:397\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    395\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[0;32m    396\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 397\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    398\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    399\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[0;32m    400\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [1563, 6250]"
     ]
    }
   ],
   "execution_count": 217
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:03:04.686765Z",
     "start_time": "2024-10-21T20:03:04.672897Z"
    }
   },
   "source": [
    "# Step 3.3: Verify your model using the testing dataset and appropriate evaluation metrics with the new merged dataset (Forecasted Data)\n",
    "y_large_pred_closed_form = X_large_sample_forecast_with_bias @ theta_large_sample\n",
    "# print(y_large_pred_closed_form)\n",
    "\n",
    "mse = mean_squared_error(y_large_test_sample, y_large_pred_closed_form)\n",
    "mae = mean_absolute_error(y_large_test_sample, y_large_pred_closed_form)\n",
    "r2 = r2_score(y_large_test_sample, y_large_pred_closed_form)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Step 3.3: Model evaluation on the testing dataset:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.5f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.5f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.5f}\")\n",
    "print(f\"R-squared: {r2:.5f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3.3: Model evaluation on the testing dataset:\n",
      "Root Mean Squared Error (RMSE): 0.02865\n",
      "Mean Squared Error (MSE): 0.00082\n",
      "Mean Absolute Error (MAE): 0.02156\n",
      "R-squared: 0.68197\n"
     ]
    }
   ],
   "execution_count": 216
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Non-linear Regression\n",
    "\n",
    "In Step 1's formulation, if the price $\\lambda$ is treated as a constant and the actual value p is known, the entire formula simplifies into a function dependent on the predicted value $\\hat{p}_t$. This implies that the problem can be reframed as an optimization task concerning the prediction of $\\hat{p}_t$. Given this perspective, extending the linear regression model from Step 3 by incorporating nonlinear features to predict $\\hat{p}_t$ effectively transforms the problem into a nonlinear regression for Step 1's objective. Therefore, performing nonlinear regression on the prediction model of $\\hat{p}_t$ inherently satisfies the requirements of the nonlinear extension outlined in Step 4."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:30:39.874288Z",
     "start_time": "2024-10-21T19:30:39.867968Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score"
   ],
   "outputs": [],
   "execution_count": 151
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:03:55.710502Z",
     "start_time": "2024-10-21T20:03:55.700294Z"
    }
   },
   "source": [
    "# Define cross-validation function, which returns RMSE\n",
    "def perform_cross_validation(X, y, degree, n_splits=10):\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    # Initialize linear regression model\n",
    "    linear_model = LinearRegression()\n",
    "    \n",
    "    # Use KFold for cross-validation, n_splits set to 10\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Compute RMSE via cross-validation (use negative MSE and then take the square root)\n",
    "    neg_mse_scores = cross_val_score(linear_model, X_poly, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    # Convert negative MSE to RMSE\n",
    "    rmse_scores = np.sqrt(-neg_mse_scores)\n",
    "    \n",
    "    # Return the mean RMSE\n",
    "    return rmse_scores.mean()"
   ],
   "outputs": [],
   "execution_count": 218
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:03:57.834623Z",
     "start_time": "2024-10-21T20:03:57.181270Z"
    }
   },
   "source": [
    "# Polynomial degrees to evaluate: 2, 3, and 4\n",
    "degrees = [2, 3, 4]\n",
    "best_degree = None\n",
    "best_rmse = float('inf')\n",
    "\n",
    "# Perform cross-validation for each polynomial degree\n",
    "for degree in degrees:\n",
    "    print(f\"Evaluating degree {degree} polynomial: \")\n",
    "    rmse = perform_cross_validation(features, data[target_column], degree)\n",
    "    print(f\"Mean RMSE for degree {degree}: {rmse:.5f}\")\n",
    "    \n",
    "    # Update the best model based on RMSE\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_degree = degree"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating degree 2 polynomial: \n",
      "Mean RMSE for degree 2: 0.02963\n",
      "Evaluating degree 3 polynomial: \n",
      "Mean RMSE for degree 3: 0.02762\n",
      "Evaluating degree 4 polynomial: \n",
      "Mean RMSE for degree 4: 0.02726\n"
     ]
    }
   ],
   "execution_count": 219
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:04:01.764370Z",
     "start_time": "2024-10-21T20:04:01.756964Z"
    }
   },
   "source": [
    "print(f\"Best degree is {best_degree} with RMSE: {best_rmse:.5f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best degree is 4 with RMSE: 0.02726\n"
     ]
    }
   ],
   "execution_count": 220
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:06:30.551194Z",
     "start_time": "2024-10-21T20:06:30.487554Z"
    }
   },
   "source": [
    "# Generate the best polynomial features\n",
    "poly_best = PolynomialFeatures(degree=best_degree, include_bias=False)\n",
    "X_poly_best = poly_best.fit_transform(features)\n",
    "# Split the data into training and testing sets\n",
    "X_train_poly_best, X_test_poly_best, y_train_poly_best, y_test_poly_best = train_test_split(\n",
    "    X_poly_best, data[target_column], test_size=0.2, random_state=42)\n",
    "\n",
    "X_forecast_poly_best = data2[data2.index.isin(pd.to_datetime(y_test_poly_best.index))]\n",
    "X_forecast_poly_best = X_forecast_poly_best.reindex(y_test_poly_best.index)\n",
    "X_forecast_poly_best = poly_best.fit_transform(X_forecast_poly_best)\n",
    "\n",
    "# Train the final polynomial model\n",
    "linear_model_best = LinearRegression()\n",
    "linear_model_best.fit(X_train_poly_best, y_train_poly_best)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_poly_best = linear_model_best.predict(X_forecast_poly_best)\n",
    "\n",
    "test_rmse_best = np.sqrt(mean_squared_error(y_test_poly_best, y_pred_poly_best))\n",
    "test_mae_best = mean_absolute_error(y_test_poly_best, y_pred_poly_best)\n",
    "test_r2_best = r2_score(y_test_poly_best, y_pred_poly_best)\n",
    "\n",
    "# Evaluation\n",
    "print(f\"Test RMSE for the best degree {best_degree}: {test_rmse_best:.5f}\")\n",
    "print(f\"Test MAE for the best degree {best_degree}: {test_mae_best:.5f}\")\n",
    "print(f\"Test R-squared for the best degree {best_degree}: {test_r2_best:.5f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE for the best degree 4: 0.02731\n",
      "Test MAE for the best degree 4: 0.02010\n",
      "Test R-squared for the best degree 4: 0.71101\n"
     ]
    }
   ],
   "execution_count": 221
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:06:37.280966Z",
     "start_time": "2024-10-21T20:06:37.270571Z"
    }
   },
   "cell_type": "code",
   "source": "X_forecast_poly_best",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.63623645e-01, -1.30353379e-01, -8.90381571e-01, ...,\n",
       "         1.64280257e-01, -8.39896431e-02,  4.29404012e-02],\n",
       "       [-2.58734779e-01, -6.26248055e-01,  9.64713103e-01, ...,\n",
       "         6.45221700e-02, -1.76103137e-02,  4.80645876e-03],\n",
       "       [ 1.02720390e+00, -6.59086645e-01, -9.94774907e-01, ...,\n",
       "         1.03142473e-02, -1.05853851e-03,  1.08636506e-04],\n",
       "       ...,\n",
       "       [-3.39004191e-01, -1.20029381e+00, -8.66021942e-01, ...,\n",
       "         1.87502998e-01, -1.08256638e-01,  6.25029983e-02],\n",
       "       [-1.09539722e+00,  1.06304503e+00,  7.43965813e-01, ...,\n",
       "         2.47139341e-01,  2.21976433e-01,  1.99375528e-01],\n",
       "       [-1.36100716e+00, -3.71022017e-01,  9.17114767e-01, ...,\n",
       "         1.33651134e-01,  5.80913671e-02,  2.52493700e-02]])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 222
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:06:49.151322Z",
     "start_time": "2024-10-21T20:06:49.142194Z"
    }
   },
   "source": [
    "# Extract coefficients\n",
    "coefficients = linear_model_best.coef_\n",
    "intercept = linear_model_best.intercept_\n",
    "\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Intercept:\", intercept)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-1.48867315e-03 -9.19752561e+07  7.04248170e+08 -7.84160137e+06\n",
      " -1.46478090e+09 -1.24663952e+08  3.67563983e+07 -1.25266327e+08\n",
      "  2.11580260e+09  1.17807263e+08 -2.32005841e+07 -1.17711665e+09\n",
      " -9.68322864e+07  2.32460878e+09  1.47390366e-03  1.67107582e-03\n",
      "  2.52133608e-03  4.61405516e-03 -1.64717436e-04 -2.95412540e-03\n",
      "  1.47290528e-03 -4.43306565e-03  1.96750462e-03  2.93576717e-03\n",
      "  3.63379717e-03 -3.90475988e-03  3.44319735e-03  9.19752560e+07\n",
      " -4.42372635e-04  9.19752560e+07 -7.04248170e+08  7.84160136e+06\n",
      " -7.04248170e+08  7.84160138e+06 -1.31431222e-03  3.89933586e-04\n",
      " -1.68734789e-03 -2.32326984e-03 -9.39011574e-04  1.79940462e-03\n",
      "  3.82706523e-03  1.46478090e+09  3.02664936e-03  1.46478090e+09\n",
      "  3.90529633e-04  1.36315823e-03  1.40300021e-03  1.24663952e+08\n",
      " -5.46116382e-04  1.24663952e+08 -3.67563983e+07  1.25266327e+08\n",
      " -3.67563983e+07  1.25266327e+08 -4.48942184e-04 -1.35457516e-03\n",
      " -1.75362825e-03 -2.11580260e+09 -4.39394265e-03 -2.11580260e+09\n",
      " -1.17807263e+08  2.32005841e+07 -1.17807263e+08  2.32005842e+07\n",
      "  3.27584691e+09  9.68322864e+07  3.04996838e+09  9.68322864e+07\n",
      " -2.25878527e+08]\n",
      "Intercept: -2098730256.0799713\n"
     ]
    }
   ],
   "execution_count": 223
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For step 4.2, the method of locally weighted least squares will be used, as tought in the lecture. Different kernels will be compared and the best one will be chosen based on evaluating the performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:06:53.016526Z",
     "start_time": "2024-10-21T20:06:53.006718Z"
    }
   },
   "source": [
    "def gaussian(t):\n",
    "    return np.exp(-0.5 * t**2) / np.sqrt(2 * np.pi)\n",
    "\n",
    "def epanechnikov(t):\n",
    "    res = np.zeros_like(t)\n",
    "    res[np.abs(t) <= 1] = 0.75 * (1 - t[np.abs(t) <= 1]**2)\n",
    "    return res\n",
    "\n",
    "def tricube(t):\n",
    "    res = np.zeros_like(t)\n",
    "    res[np.abs(t) <= 1] = (70 / 81) * (1 - np.abs(t[np.abs(t) <= 1])**3)**3\n",
    "    return res\n",
    "\n",
    "def uniform(t, p=0.2):\n",
    "    return np.zeros_like(t) + p\n",
    "\n",
    "def triangle(t):\n",
    "    res = np.zeros_like(t)\n",
    "    res[np.abs(t) <= 1] = 1 - np.abs(t[np.abs(t) <= 1])\n",
    "    return res"
   ],
   "outputs": [],
   "execution_count": 224
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:06:53.701131Z",
     "start_time": "2024-10-21T20:06:53.690904Z"
    }
   },
   "source": [
    "# Locally Weighted Least Squares implementation\n",
    "def lwls_predict(X_train, y_train, X_test, kernel_func, tau=0.1):\n",
    "    y_pred = np.zeros(len(X_test))\n",
    "\n",
    "    for i, x in enumerate(X_test):\n",
    "        distances = np.linalg.norm(X_train - x, axis=1)  # Compute distances\n",
    "        weights = kernel_func(distances / tau)  # Apply kernel function\n",
    "        W = np.diag(weights)  # Create diagonal weight matrix\n",
    "\n",
    "        # Weighted Least Squares computation\n",
    "        XTWX = X_train.T @ W @ X_train  # X^T W X\n",
    "        XTWy = X_train.T @ W @ y_train  # X^T W y\n",
    "\n",
    "        # Use np.linalg.pinv for numerical stability\n",
    "        theta = np.linalg.pinv(XTWX) @ XTWy\n",
    "\n",
    "        # Ensure x is 2D before matrix multiplication\n",
    "        y_pred[i] = np.dot(x, theta)  # Prediction for the current test sample\n",
    "\n",
    "    return y_pred"
   ],
   "outputs": [],
   "execution_count": 225
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:13:14.730648Z",
     "start_time": "2024-10-21T19:13:14.722338Z"
    }
   },
   "source": [
    "# Function to evaluate different kernels and select the best one\n",
    "def evaluate_kernels(X_train, y_train, X_test, y_test, kernels, tau=0.1):\n",
    "    mse_results = {}\n",
    "\n",
    "    for kernel_name, kernel_func in kernels.items():\n",
    "\n",
    "        y_pred = lwls_predict(X_train, y_train, X_test, kernel_func, tau=tau)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_results[kernel_name] = mse\n",
    "\n",
    "    return mse_results\n",
    "\n",
    "# Example kernels (ensure these are defined somewhere in the code)\n",
    "kernels = {\n",
    "    'Gaussian': gaussian,\n",
    "    'Epanechnikov': epanechnikov,\n",
    "    'Tricube': tricube,\n",
    "    'Uniform': uniform,\n",
    "    'Triangle': triangle\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 114
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:07:35.868569Z",
     "start_time": "2024-10-21T20:07:35.797389Z"
    }
   },
   "source": [
    "# Evaluate kernels on smaller data for faster results\n",
    "mse_results_small = evaluate_kernels(X_sample_train_with_bias, y_sample_train, X_sample_test_with_bias, y_sample_test, kernels)\n",
    "min_kernel, min_mse = min(mse_results_small.items(), key=lambda x: x[1])\n",
    "\n",
    "# Evaluate kernels on smaller data from the Forecasted Dataset for faster results\n",
    "mse_results_small = evaluate_kernels(X_sample_train_with_bias, y_sample_train, X_forecast_sample_with_bias, y_sample_test, kernels)\n",
    "\n",
    "# Print the full results and the minimum one\n",
    "print(mse_results_small)\n",
    "print(f\"The kernel with the smallest MSE is '{min_kernel}' with a value of {min_mse}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gaussian': 0.006513347544134409, 'Epanechnikov': 0.014416444258954303, 'Tricube': 0.014416444258954293, 'Uniform': 0.0007615573895891005, 'Triangle': 0.014416444258954303}\n",
      "The kernel with the smallest MSE is 'Uniform' with a value of 0.0009889360972607534\n"
     ]
    }
   ],
   "execution_count": 226
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:08:16.456310Z",
     "start_time": "2024-10-21T20:08:12.495035Z"
    }
   },
   "source": [
    "# Evaluation on the kernel selected\n",
    "def evaluate_uniform(X_train, y_train, X_test, y_test, uniform, tau=0.1):\n",
    "    for kernel_name, kernel_func in uniform.items():\n",
    "        y_pred = lwls_predict(X_train, y_train, X_test, kernel_func, tau=tau)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae_wls_uniform = mean_absolute_error(y_test, y_pred)\n",
    "        r2_wls_uniform = r2_score(y_test, y_pred)\n",
    "        rmse_wls_uniform = np.sqrt(mse)\n",
    "        \n",
    "        print(f\"{kernel_name} Kernel Results:\")\n",
    "        print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "        print(f\"Mean Absolute Error (MAE): {mae_wls_uniform}\")\n",
    "        print(f\"R-squared (R2): {r2_wls_uniform}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse_wls_uniform}\")\n",
    "\n",
    "    return\n",
    "\n",
    "uniform_kernel = {\n",
    "    'Uniform': uniform\n",
    "}\n",
    "\n",
    "#evaluate_uniform(X_large_sample_with_bias, y_large_sample, X_large_test_sample_with_bias, y_large_test_sample, uniform_kernel)\n",
    "\n",
    "#Evaluation of the Kernel using the Forecasted Data\n",
    "evaluate_uniform(X_large_sample_with_bias, y_large_sample, merged_ds_with_bias, y_large_test_sample, uniform_kernel)"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1563, 20]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[227], line 26\u001B[0m\n\u001B[0;32m     19\u001B[0m uniform_kernel \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUniform\u001B[39m\u001B[38;5;124m'\u001B[39m: uniform\n\u001B[0;32m     21\u001B[0m }\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m#evaluate_uniform(X_large_sample_with_bias, y_large_sample, X_large_test_sample_with_bias, y_large_test_sample, uniform_kernel)\u001B[39;00m\n\u001B[0;32m     24\u001B[0m \n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m#Evaluation of the Kernel using the Forecasted Data\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m evaluate_uniform(X_large_sample_with_bias, y_large_sample, X_forecast_sample_with_bias, y_large_test_sample, uniform_kernel)\n",
      "Cell \u001B[1;32mIn[227], line 6\u001B[0m, in \u001B[0;36mevaluate_uniform\u001B[1;34m(X_train, y_train, X_test, y_test, uniform, tau)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m kernel_name, kernel_func \u001B[38;5;129;01min\u001B[39;00m uniform\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m      4\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m lwls_predict(X_train, y_train, X_test, kernel_func, tau\u001B[38;5;241m=\u001B[39mtau)\n\u001B[1;32m----> 6\u001B[0m     mse \u001B[38;5;241m=\u001B[39m mean_squared_error(y_test, y_pred)\n\u001B[0;32m      7\u001B[0m     mae_wls_uniform \u001B[38;5;241m=\u001B[39m mean_absolute_error(y_test, y_pred)\n\u001B[0;32m      8\u001B[0m     r2_wls_uniform \u001B[38;5;241m=\u001B[39m r2_score(y_test, y_pred)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:442\u001B[0m, in \u001B[0;36mmean_squared_error\u001B[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001B[0m\n\u001B[0;32m    382\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmean_squared_error\u001B[39m(\n\u001B[0;32m    383\u001B[0m     y_true, y_pred, \u001B[38;5;241m*\u001B[39m, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, multioutput\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muniform_average\u001B[39m\u001B[38;5;124m\"\u001B[39m, squared\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    384\u001B[0m ):\n\u001B[0;32m    385\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Mean squared error regression loss.\u001B[39;00m\n\u001B[0;32m    386\u001B[0m \n\u001B[0;32m    387\u001B[0m \u001B[38;5;124;03m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    440\u001B[0m \u001B[38;5;124;03m    0.825...\u001B[39;00m\n\u001B[0;32m    441\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 442\u001B[0m     y_type, y_true, y_pred, multioutput \u001B[38;5;241m=\u001B[39m _check_reg_targets(\n\u001B[0;32m    443\u001B[0m         y_true, y_pred, multioutput\n\u001B[0;32m    444\u001B[0m     )\n\u001B[0;32m    445\u001B[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001B[0;32m    446\u001B[0m     output_errors \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39maverage((y_true \u001B[38;5;241m-\u001B[39m y_pred) \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, weights\u001B[38;5;241m=\u001B[39msample_weight)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:100\u001B[0m, in \u001B[0;36m_check_reg_targets\u001B[1;34m(y_true, y_pred, multioutput, dtype)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_reg_targets\u001B[39m(y_true, y_pred, multioutput, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumeric\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m     67\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001B[39;00m\n\u001B[0;32m     68\u001B[0m \n\u001B[0;32m     69\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;124;03m        correct keyword.\u001B[39;00m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 100\u001B[0m     check_consistent_length(y_true, y_pred)\n\u001B[0;32m    101\u001B[0m     y_true \u001B[38;5;241m=\u001B[39m check_array(y_true, ensure_2d\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[0;32m    102\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m check_array(y_pred, ensure_2d\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39mdtype)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:397\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    395\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[0;32m    396\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 397\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    398\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    399\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[0;32m    400\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [1563, 20]"
     ]
    }
   ],
   "execution_count": 227
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 Regularization\n",
    "Ridge and Lasso regression are applied to test if the variance of the dataset can and has to be improved. Applying one of these techniques could make the model more stable and improve the prediction. \n",
    "Different alpha values are tested to see which one results in the best results. Both for Lasso and Ridge the goal is to minimize the mean squared error. To find the optimal alpha values GridSearchCV is used, which applies 5-fold cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:08:50.985470Z",
     "start_time": "2024-10-21T20:08:50.612909Z"
    }
   },
   "source": [
    "# Create a list with possible alpha values to iterate over\n",
    "alpha_values = {'alpha': [0.0001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Ridge model with cross-validation\n",
    "ridge_model = Ridge()\n",
    "# Apply GridSearchCV to search for the optimal alpha\n",
    "ridge_cv = GridSearchCV(ridge_model, param_grid=alpha_values, cv=5, scoring='neg_mean_squared_error') # cv=5 for 5-fold cross-validation, scoring mean squared error because the goal is to get this as low as possible\n",
    "ridge_cv.fit(X_large_sample,y_large_sample)\n",
    "#ridge_cv.fit(merged_ds,y_large_test_sample)\n",
    "\n",
    "# Lasso model with cross-validation\n",
    "lasso_model = Lasso()\n",
    "# Apply GridSearchCV to search for the optimal alpha\n",
    "lasso_cv = GridSearchCV(lasso_model, param_grid=alpha_values, cv=5, scoring='neg_mean_squared_error') # cv=5 for 5-fold cross-validation, scoring mean squared error because the goal is to get this as low as possible\n",
    "# lasso_cv.fit(X_large_test_sample,y_large_test_sample)\n",
    "lasso_cv.fit(X_large_sample,y_large_sample)\n",
    "\n",
    "# Get the best alpha values\n",
    "best_alpha_ridge = ridge_cv.best_params_['alpha']\n",
    "best_alpha_lasso = lasso_cv.best_params_['alpha']\n",
    "\n",
    "print(f\"Optimal alpha for Ridge with the full dataset: {best_alpha_ridge}\")\n",
    "print(f\"Optimal alpha for Lasso with the full dataset: {best_alpha_lasso}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha for Ridge with the full dataset: 1\n",
      "Optimal alpha for Lasso with the full dataset: 0.0001\n"
     ]
    }
   ],
   "execution_count": 228
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:09:06.824134Z",
     "start_time": "2024-10-21T20:09:06.805791Z"
    }
   },
   "source": [
    "# Run the lasso model with the optimal alpha for the new Forecasted Dataset\n",
    "lasso_model = Lasso(alpha=best_alpha_lasso)\n",
    "lasso_model.fit(X_large_sample,y_large_sample)\n",
    "# Predict the power production with Lasso regularization\n",
    "y_pred_lasso = lasso_model.predict(X_large_sample_forecast)\n",
    "# Show the new coefficients\n",
    "lasso_model.coef_"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.46137885e-05, -3.90481705e-02,  5.75722061e-03,  3.39852023e-03])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 230
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:09:10.382609Z",
     "start_time": "2024-10-21T20:09:10.372455Z"
    }
   },
   "source": [
    "# Verify the model using the testing dataset and appropriate evaluation metrics\n",
    "mse_lasso = mean_squared_error(y_large_test_sample, y_pred_lasso)\n",
    "mae_lasso= mean_absolute_error(y_large_test_sample, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_large_test_sample, y_pred_lasso)\n",
    "rmse_lasso = np.sqrt(mse_lasso)\n",
    "\n",
    "print(f\"Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 0.0001:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_lasso:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_lasso:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_lasso:.4f}\")\n",
    "print(f\"R-squared: {r2_lasso:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 0.0001:\n",
      "Root Mean Squared Error (RMSE): 0.0286\n",
      "Mean Squared Error (MSE): 0.0008\n",
      "Mean Absolute Error (MAE): 0.0216\n",
      "R-squared: 0.6821\n"
     ]
    }
   ],
   "execution_count": 231
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:09:17.217546Z",
     "start_time": "2024-10-21T20:09:17.183012Z"
    }
   },
   "source": [
    "# Run the ridge model with the optimal alpha\n",
    "ridge_model = Ridge(alpha=best_alpha_ridge)\n",
    "ridge_model.fit(X_large_sample,y_large_sample)\n",
    "# Predict the power production with Ridge regularization\n",
    "y_pred_ridge = ridge_model.predict(X_large_sample_forecast)\n",
    "# Show the new coefficients\n",
    "ridge_model.coef_"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00015682, -0.03908494,  0.00590483,  0.00365619])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 232
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:09:21.132916Z",
     "start_time": "2024-10-21T20:09:21.115680Z"
    }
   },
   "source": [
    "# Verify the model using the testing dataset and appropriate evaluation metrics\n",
    "mse_ridge = mean_squared_error(y_large_test_sample, y_pred_lasso)\n",
    "mae_ridge= mean_absolute_error(y_large_test_sample, y_pred_lasso)\n",
    "r2_ridge = r2_score(y_large_test_sample, y_pred_lasso)\n",
    "rmse_ridge = np.sqrt(mse_lasso)\n",
    "\n",
    "print(f\"Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 1:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_ridge:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_ridge:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_ridge:.4f}\")\n",
    "print(f\"R-squared: {r2_ridge:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 1:\n",
      "Root Mean Squared Error (RMSE): 0.0286\n",
      "Mean Squared Error (MSE): 0.0008\n",
      "Mean Absolute Error (MAE): 0.0216\n",
      "R-squared: 0.6821\n"
     ]
    }
   ],
   "execution_count": 233
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:09:27.275186Z",
     "start_time": "2024-10-21T20:09:27.259828Z"
    }
   },
   "source": [
    "# Run the ridge model with the optimal alpha\n",
    "ridge_model = Ridge(alpha=0.0001)\n",
    "ridge_model.fit(X_large_sample,y_large_sample)\n",
    "# Predict the power production with Ridge regularization\n",
    "y_pred_ridge = ridge_model.predict(X_large_sample_forecast)\n",
    "# Show the new coefficients\n",
    "ridge_model.coef_"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00015603, -0.03909132,  0.00590537,  0.00365567])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 234
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:09:30.761820Z",
     "start_time": "2024-10-21T20:09:30.750112Z"
    }
   },
   "source": [
    "# Verify the model using the testing dataset and appropriate evaluation metrics\n",
    "mse_ridge = mean_squared_error(y_large_test_sample, y_pred_lasso)\n",
    "mae_ridge= mean_absolute_error(y_large_test_sample, y_pred_lasso)\n",
    "r2_ridge = r2_score(y_large_test_sample, y_pred_lasso)\n",
    "rmse_ridge = np.sqrt(mse_lasso)\n",
    "\n",
    "print(f\"Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 1:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_ridge:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_ridge:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_ridge:.4f}\")\n",
    "print(f\"R-squared: {r2_ridge:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 1:\n",
      "Root Mean Squared Error (RMSE): 0.0286\n",
      "Mean Squared Error (MSE): 0.0008\n",
      "Mean Absolute Error (MAE): 0.0216\n",
      "R-squared: 0.6821\n"
     ]
    }
   ],
   "execution_count": 235
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal penalty term for Lasso regression is 0.0001. This results in validation metric values with almost the same results as normal regression. Moreover, such a small penalty term indicates that it would be better to simply apply normal regression. This makes sense because for this model n>>p. There is a very large amount of data points and only 4 parameters. Consequently, there is already very low variance, before regularization is applied, minimizing the need for additional regularization. \n",
    "\n",
    "Ridge regression shows similar results, with one notable difference. For Ridge regression, it does not matter whether the penalty term is set to 1 or to 0.0001. The validation metrics remain exactly the same. This indicates that the data is inherently regularized; there is already a very low variance and the Ridge regression is not required to improve results. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:09:47.487287Z",
     "start_time": "2024-10-21T20:09:43.835137Z"
    }
   },
   "source": [
    "# Create a list with possible alpha values to iterate over\n",
    "alpha_values = {'alpha': [0.0001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Ridge model with cross-validation\n",
    "ridge_model_poly = Ridge()\n",
    "# Apply GridSearchCV to search for the optimal alpha\n",
    "ridge_cv_poly = GridSearchCV(ridge_model_poly, param_grid=alpha_values, cv=5, scoring='neg_mean_squared_error') # cv=5 for 5-fold cross-validation, scoring mean squared error because the goal is to get this as low as possible\n",
    "ridge_cv_poly.fit(X_train_poly_best,y_train_poly_best)\n",
    "#ridge_cv.fit(merged_ds,y_large_test_sample)\n",
    "\n",
    "# Lasso model with cross-validation\n",
    "lasso_model_poly = Lasso()\n",
    "# Apply GridSearchCV to search for the optimal alpha\n",
    "lasso_cv_poly = GridSearchCV(lasso_model_poly, param_grid=alpha_values, cv=5, scoring='neg_mean_squared_error') # cv=5 for 5-fold cross-validation, scoring mean squared error because the goal is to get this as low as possible\n",
    "# lasso_cv.fit(X_large_test_sample,y_large_test_sample)\n",
    "lasso_cv_poly.fit(X_train_poly_best,y_train_poly_best)\n",
    "\n",
    "# Get the best alpha values\n",
    "best_alpha_ridge_poly = ridge_cv.best_params_['alpha']\n",
    "best_alpha_lasso_poly = lasso_cv.best_params_['alpha']\n",
    "\n",
    "print(f\"Optimal alpha for Ridge with the full dataset for polynomial regression: {best_alpha_ridge_poly}\")\n",
    "print(f\"Optimal alpha for Lasso with the full dataset for polynomial regression: {best_alpha_lasso_poly}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha for Ridge with the full dataset for polynomial regression: 1\n",
      "Optimal alpha for Lasso with the full dataset for polynomial regression: 0.0001\n"
     ]
    }
   ],
   "execution_count": 236
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:11:22.646320Z",
     "start_time": "2024-10-21T20:11:22.295585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the lasso model with the optimal alpha for the new Forecasted Dataset\n",
    "lasso_model_poly = Lasso(alpha=best_alpha_lasso)\n",
    "lasso_model_poly.fit(X_train_poly_best,y_train_poly_best)\n",
    "# Predict the power production with Lasso regularization\n",
    "y_pred_lasso_poly = lasso_model_poly.predict(X_forecast_poly_best)\n"
   ],
   "outputs": [],
   "execution_count": 237
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:11:24.204258Z",
     "start_time": "2024-10-21T20:11:24.191646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verify the model using the testing dataset and appropriate evaluation metrics\n",
    "mse_lasso_poly = mean_squared_error(y_test_poly_best, y_pred_lasso_poly)\n",
    "mae_lasso_poly= mean_absolute_error(y_test_poly_best, y_pred_lasso_poly)\n",
    "r2_lasso_poly = r2_score(y_test_poly_best, y_pred_lasso_poly)\n",
    "rmse_lasso_poly = np.sqrt(mse_lasso_poly)\n",
    "\n",
    "print(f\"Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 0.0001:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_lasso_poly:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_lasso_poly:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_lasso_poly:.4f}\")\n",
    "print(f\"R-squared: {r2_lasso_poly:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 0.0001:\n",
      "Root Mean Squared Error (RMSE): 0.0269\n",
      "Mean Squared Error (MSE): 0.0007\n",
      "Mean Absolute Error (MAE): 0.0197\n",
      "R-squared: 0.7189\n"
     ]
    }
   ],
   "execution_count": 238
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:11:39.129474Z",
     "start_time": "2024-10-21T20:11:38.716662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the lasso model with the optimal alpha for the new Forecasted Dataset\n",
    "ridge_model_poly = Lasso(alpha=best_alpha_lasso)\n",
    "ridge_model_poly.fit(X_train_poly_best,y_train_poly_best)\n",
    "# Predict the power production with Lasso regularization\n",
    "y_pred_ridge_poly = ridge_model_poly.predict(X_forecast_poly_best)\n"
   ],
   "outputs": [],
   "execution_count": 239
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T20:11:41.173607Z",
     "start_time": "2024-10-21T20:11:41.161219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verify the model using the testing dataset and appropriate evaluation metrics\n",
    "mse_ridge_poly = mean_squared_error(y_test_poly_best, y_pred_ridge_poly)\n",
    "mae_ridge_poly= mean_absolute_error(y_test_poly_best, y_pred_ridge_poly)\n",
    "r2_ridge_poly = r2_score(y_test_poly_best, y_pred_ridge_poly)\n",
    "rmse_ridge_poly = np.sqrt(mse_ridge_poly)\n",
    "\n",
    "print(f\"Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 0.0001:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_ridge_poly:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_ridge_poly:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_ridge_poly:.4f}\")\n",
    "print(f\"R-squared: {r2_ridge_poly:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 0.0001:\n",
      "Root Mean Squared Error (RMSE): 0.0269\n",
      "Mean Squared Error (MSE): 0.0007\n",
      "Mean Absolute Error (MAE): 0.0197\n",
      "R-squared: 0.7189\n"
     ]
    }
   ],
   "execution_count": 240
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
