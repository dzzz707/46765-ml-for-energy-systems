{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T10:28:31.210393Z",
     "start_time": "2024-10-15T10:28:31.199915Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV"
   ],
   "outputs": [],
   "execution_count": 119
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T10:28:33.591292Z",
     "start_time": "2024-10-15T10:28:33.526825Z"
    }
   },
   "source": [
    "file_path = 'Data assignment 1/Feature data.csv'\n",
    "data = pd.read_csv(file_path)"
   ],
   "outputs": [],
   "execution_count": 120
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step the different features are scaled, in order to make sure the models can interpret the features on a similar scale. The wind speed and the maximum temperature undergo the standardscaler, while wind direction is converted to sinus and cosinus components. The power production is normalized using the nominal capacity of 30 MW (https://stateofgreen.com/en/solutions/kalby-wind-turbines/). \n",
    " "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T10:28:36.353421Z",
     "start_time": "2024-10-15T10:28:36.304077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import required scalers\n",
    "scaler_standard = StandardScaler()\n",
    "\n",
    "### 1. Standard Scaling for wind speed and temperature\n",
    "data['Mean wind speed'] = scaler_standard.fit_transform(data[['Mean wind speed']])\n",
    "data['Maximum temperature'] = scaler_standard.fit_transform(data[['Maximum temperature']])\n",
    "\n",
    "### 2. Wind Direction (convert to sin and cos components)\n",
    "data['Wind direction sin'] = np.sin(np.deg2rad(data['Mean wind direction']))\n",
    "data['Wind direction cos'] = np.cos(np.deg2rad(data['Mean wind direction']))\n",
    "\n",
    "### 3. Normalize Power Production \n",
    "nominal_capacity = 30000 # production capacity is 30 MW, unit of power production is kW so nominal capacity is 30000 (kW)\n",
    "data['AKI Kalby Active Power'] = data['AKI Kalby Active Power'] / nominal_capacity\n",
    "\n",
    "# Dropping the original wind direction after scaling\n",
    "data = data.drop('Mean wind direction', axis=1)"
   ],
   "outputs": [],
   "execution_count": 121
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T10:28:38.642843Z",
     "start_time": "2024-10-15T10:28:38.598070Z"
    }
   },
   "source": [
    "# Make sure datetime is set as the index\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "data.set_index('datetime', inplace=True)"
   ],
   "outputs": [],
   "execution_count": 122
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T10:28:39.016577Z",
     "start_time": "2024-10-15T10:28:39.007306Z"
    }
   },
   "source": [
    "# Set target and features, and remove non-numeric columns\n",
    "target_column = 'AKI Kalby Active Power'\n",
    "features = data.select_dtypes(include=[np.number]).drop(columns=[target_column])"
   ],
   "outputs": [],
   "execution_count": 123
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T10:28:39.948455Z",
     "start_time": "2024-10-15T10:28:39.932134Z"
    }
   },
   "source": [
    "# Load the dataframe to check if scaling worked\n",
    "data.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                     AKI Kalby Active Power  Maximum temperature  \\\n",
       "datetime                                                           \n",
       "2022-01-01 00:00:00               -0.063118            -0.457945   \n",
       "2022-01-01 01:00:00               -0.055728            -0.457945   \n",
       "2022-01-01 02:00:00               -0.095724            -0.503187   \n",
       "2022-01-01 03:00:00               -0.063726            -0.518268   \n",
       "2022-01-01 04:00:00               -0.029392            -0.473025   \n",
       "\n",
       "                     Mean wind speed  Wind direction sin  Wind direction cos  \n",
       "datetime                                                                      \n",
       "2022-01-01 00:00:00         0.868655           -0.998630       -5.233596e-02  \n",
       "2022-01-01 01:00:00         0.382418           -0.956305       -2.923717e-01  \n",
       "2022-01-01 02:00:00         0.756447           -0.994522       -1.045285e-01  \n",
       "2022-01-01 03:00:00         0.494627           -1.000000       -1.836970e-16  \n",
       "2022-01-01 04:00:00         0.307612           -0.951057        3.090170e-01  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AKI Kalby Active Power</th>\n",
       "      <th>Maximum temperature</th>\n",
       "      <th>Mean wind speed</th>\n",
       "      <th>Wind direction sin</th>\n",
       "      <th>Wind direction cos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:00:00</th>\n",
       "      <td>-0.063118</td>\n",
       "      <td>-0.457945</td>\n",
       "      <td>0.868655</td>\n",
       "      <td>-0.998630</td>\n",
       "      <td>-5.233596e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 01:00:00</th>\n",
       "      <td>-0.055728</td>\n",
       "      <td>-0.457945</td>\n",
       "      <td>0.382418</td>\n",
       "      <td>-0.956305</td>\n",
       "      <td>-2.923717e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 02:00:00</th>\n",
       "      <td>-0.095724</td>\n",
       "      <td>-0.503187</td>\n",
       "      <td>0.756447</td>\n",
       "      <td>-0.994522</td>\n",
       "      <td>-1.045285e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 03:00:00</th>\n",
       "      <td>-0.063726</td>\n",
       "      <td>-0.518268</td>\n",
       "      <td>0.494627</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 04:00:00</th>\n",
       "      <td>-0.029392</td>\n",
       "      <td>-0.473025</td>\n",
       "      <td>0.307612</td>\n",
       "      <td>-0.951057</td>\n",
       "      <td>3.090170e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Constructing testing and training \n",
    "Using train_test_split the data is split into testing and training data. The choice was made to not use TimeSeriesSplit, because although the data is time based, the values are not dependent on the time of day in the sense that there is no strong temporal relationship that affects the observations. The power production is more weather dependent than anything else. \n",
    " The data points can be treated independently of their time indices, allowing for a standard random sampling approach. By maintaining a randomized split, we also prevent potential biases that could arise from time-based sequences, ensuring that both the training and testing set represent the overall distribution of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:55.754383Z",
     "start_time": "2024-10-15T08:58:55.731495Z"
    }
   },
   "source": [
    "# Split the data\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "# Select a 100 datapoints as a start with a low number of samples\n",
    "X_sample = features[:100]\n",
    "y_sample = data[target_column][:100]\n",
    "\n",
    "# Sequential split (shuffle=False)\n",
    "# Give the random_state a set seed of 42, to ensure that the split will be the same everytime in order to reproduce results\n",
    "X_sample_train, X_sample_test, y_sample_train, y_sample_test = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.2, shuffle=False, random_state=42)\n",
    "\n",
    "# Adding a column of ones to X_sample for the bias term and converting to NumPy array\n",
    "X_sample_train_with_bias = np.c_[np.ones(X_sample_train.shape[0]), X_sample_train].astype(float)\n",
    "X_sample_test_with_bias = np.c_[np.ones(X_sample_test.shape[0]), X_sample_test].astype(float)\n",
    "\n",
    "# Ensure y_sample_train is also a NumPy array\n",
    "y_sample_train = np.array(y_sample_train).astype(float)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN indices: [   0    1    2 ... 6248 6249 6250] TEST indices: [6251 6252 6253 ... 7810 7811 7812]\n",
      "X_train shape: (6251, 4)\n",
      "X_test shape: (1562, 4)\n",
      "y_train shape: (6251,)\n",
      "y_test shape: (1562,)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.233572Z",
     "start_time": "2024-10-15T08:58:55.778270Z"
    }
   },
   "source": [
    "# Gradient Descent function\n",
    "def gradient_descent(X, y, learning_rate=0.01, epochs=100000):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    for _ in range(epochs):\n",
    "        y_pred = X @ theta\n",
    "        gradients = (1/m) * X.T @ (y_pred - y)\n",
    "        theta -= learning_rate * gradients\n",
    "    return theta\n",
    "\n",
    "theta_gd = gradient_descent(X_sample_train_with_bias, y_sample_train)\n",
    "\n",
    "# Predictions using Gradient Descent\n",
    "y_pred_gd = X_sample_test_with_bias @ theta_gd"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.534123Z",
     "start_time": "2024-10-15T08:58:57.233572Z"
    }
   },
   "source": [
    "# Closed-form solution \n",
    "theta_closed_form = np.linalg.inv(X_sample_train_with_bias.T @ X_sample_train_with_bias) @ X_sample_train_with_bias.T @ y_sample_train\n",
    "\n",
    "# Predictions using closed-form solution\n",
    "y_pred_closed_form = X_sample_test_with_bias @ theta_closed_form"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.547467Z",
     "start_time": "2024-10-15T08:58:57.534123Z"
    }
   },
   "source": [
    "# mse calculation\n",
    "mse_gd = mean_squared_error(y_sample_test, y_pred_gd)\n",
    "mse_closed_form = mean_squared_error(y_sample_test, y_pred_closed_form)\n",
    "\n",
    "print(f\"Gradient Descent θ: {[f'{x:.5f}' for x in theta_gd]}\")\n",
    "print(f\"Closed-Form θ: {[f'{x:.5f}' for x in theta_closed_form]}\")\n",
    "print(f\"Gradient Descent MSE: {mse_gd:.5f}\")\n",
    "print(f\"Closed-Form MSE: {mse_closed_form:.5f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent θ: ['-0.03789', '0.02592', '-0.04119', '0.01152', '0.01116']\n",
      "Closed-Form θ: ['-0.03789', '0.02592', '-0.04119', '0.01152', '0.01116']\n",
      "Gradient Descent MSE: 0.00099\n",
      "Closed-Form MSE: 0.00099\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.580788Z",
     "start_time": "2024-10-15T08:58:57.547467Z"
    }
   },
   "source": [
    "# Step 3.2: Use the full dataset and closed form solution\n",
    "X_large_sample, X_large_test_sample, y_large_sample, y_large_test_sample = train_test_split(features, data[target_column], test_size=0.2, random_state=42)\n",
    "\n",
    "# Adding a column of ones for the bias term in the large sample\n",
    "X_large_sample_with_bias = np.c_[np.ones(X_large_sample.shape[0]), X_large_sample]\n",
    "X_large_test_sample_with_bias = np.c_[np.ones(X_large_test_sample.shape[0]), X_large_test_sample]\n",
    "\n",
    "# Upgrade the normal equation\n",
    "theta_large_sample = np.linalg.inv(X_large_sample_with_bias.T @ X_large_sample_with_bias) @ X_large_sample_with_bias.T @ y_large_sample\n",
    "theta_large_sample_rounded = np.round(theta_large_sample, 5)\n",
    "\n",
    "print(f\"Step 3.2: Closed-form solution training complete on the larger sample.\")\n",
    "print(f\"Coefficients: {[f'{x:.5f}' for x in theta_large_sample_rounded]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3.2: Closed-form solution training complete on the larger sample.\n",
      "Coefficients: ['-0.04394', '0.00016', '-0.03909', '0.00591', '0.00366']\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.604789Z",
     "start_time": "2024-10-15T08:58:57.580788Z"
    }
   },
   "source": [
    "# Step 3.3: Verify your model using the testing dataset and appropriate evaluation metrics\n",
    "y_large_pred_closed_form = X_large_test_sample_with_bias @ theta_large_sample\n",
    "\n",
    "mse = mean_squared_error(y_large_test_sample, y_large_pred_closed_form)\n",
    "mae = mean_absolute_error(y_large_test_sample, y_large_pred_closed_form)\n",
    "r2 = r2_score(y_large_test_sample, y_large_pred_closed_form)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Step 3.3: Model evaluation on the testing dataset:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.5f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.5f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.5f}\")\n",
    "print(f\"R-squared: {r2:.5f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3.3: Model evaluation on the testing dataset:\n",
      "Root Mean Squared Error (RMSE): 0.03016\n",
      "Mean Squared Error (MSE): 0.00091\n",
      "Mean Absolute Error (MAE): 0.02294\n",
      "R-squared: 0.64752\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Non-linear Regression\n",
    "\n",
    "In Step 1's formulation, if the price $\\lambda$ is treated as a constant and the actual value p is known, the entire formula simplifies into a function dependent on the predicted value $\\hat{p}_t$. This implies that the problem can be reframed as an optimization task concerning the prediction of $\\hat{p}_t$. Given this perspective, extending the linear regression model from Step 3 by incorporating nonlinear features to predict $\\hat{p}_t$ effectively transforms the problem into a nonlinear regression for Step 1's objective. Therefore, performing nonlinear regression on the prediction model of $\\hat{p}_t$ inherently satisfies the requirements of the nonlinear extension outlined in Step 4."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.627386Z",
     "start_time": "2024-10-15T08:58:57.604789Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.731195Z",
     "start_time": "2024-10-15T08:58:57.627386Z"
    }
   },
   "source": [
    "# Step 4.1: Add polynomial features (squared and cubic terms)\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False) \n",
    "X_poly = poly.fit_transform(features)\n",
    "\n",
    "# Convert to DataFrame with correct column names\n",
    "X_poly_df = pd.DataFrame(X_poly, columns=poly.get_feature_names_out(features.columns))\n",
    "\n",
    "# Split the polynomial feature data\n",
    "X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(X_poly_df, data[target_column], test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit linear regression model on the polynomial features\n",
    "linear_model_poly = LinearRegression()\n",
    "linear_model_poly.fit(X_train_poly, y_train_poly)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred_poly = linear_model_poly.predict(X_test_poly)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.747447Z",
     "start_time": "2024-10-15T08:58:57.731195Z"
    }
   },
   "source": [
    "# Evaluate the performance\n",
    "mse_poly = mean_squared_error(y_test_poly, y_pred_poly)\n",
    "mae_poly = mean_absolute_error(y_test_poly, y_pred_poly)\n",
    "r2_poly = r2_score(y_test_poly, y_pred_poly)\n",
    "rmse_poly = np.sqrt(mse_poly)\n",
    "\n",
    "print(f\"Nonlinear model evaluation on the testing dataset:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_poly:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_poly:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_poly:.4f}\")\n",
    "print(f\"R-squared: {r2_poly:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonlinear model evaluation on the testing dataset:\n",
      "Root Mean Squared Error (RMSE): 0.0278\n",
      "Mean Squared Error (MSE): 0.0008\n",
      "Mean Absolute Error (MAE): 0.0204\n",
      "R-squared: 0.7014\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For step 4.2, the method of locally weighted least squares will be used, as tought in the lecture. Different kernels will be compared and the best one will be chosen based on evaluating the performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.765994Z",
     "start_time": "2024-10-15T08:58:57.747447Z"
    }
   },
   "source": [
    "def gaussian(t):\n",
    "    return np.exp(-0.5 * t**2) / np.sqrt(2 * np.pi)\n",
    "\n",
    "def epanechnikov(t):\n",
    "    res = np.zeros_like(t)\n",
    "    res[np.abs(t) <= 1] = 0.75 * (1 - t[np.abs(t) <= 1]**2)\n",
    "    return res\n",
    "\n",
    "def tricube(t):\n",
    "    res = np.zeros_like(t)\n",
    "    res[np.abs(t) <= 1] = (70 / 81) * (1 - np.abs(t[np.abs(t) <= 1])**3)**3\n",
    "    return res\n",
    "\n",
    "def uniform(t, p=0.2):\n",
    "    return np.zeros_like(t) + p\n",
    "\n",
    "def triangle(t):\n",
    "    res = np.zeros_like(t)\n",
    "    res[np.abs(t) <= 1] = 1 - np.abs(t[np.abs(t) <= 1])\n",
    "    return res"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.778638Z",
     "start_time": "2024-10-15T08:58:57.769007Z"
    }
   },
   "source": [
    "# Locally Weighted Least Squares implementation\n",
    "def lwls_predict(X_train, y_train, X_test, kernel_func, tau=0.1):\n",
    "    y_pred = np.zeros(len(X_test))\n",
    "\n",
    "    for i, x in enumerate(X_test):\n",
    "        distances = np.linalg.norm(X_train - x, axis=1)  # Compute distances\n",
    "        weights = kernel_func(distances / tau)  # Apply kernel function\n",
    "        W = np.diag(weights)  # Create diagonal weight matrix\n",
    "\n",
    "        # Weighted Least Squares computation\n",
    "        XTWX = X_train.T @ W @ X_train  # X^T W X\n",
    "        XTWy = X_train.T @ W @ y_train  # X^T W y\n",
    "\n",
    "        # Use np.linalg.pinv for numerical stability\n",
    "        theta = np.linalg.pinv(XTWX) @ XTWy\n",
    "\n",
    "        # Ensure x is 2D before matrix multiplication\n",
    "        y_pred[i] = np.dot(x, theta)  # Prediction for the current test sample\n",
    "\n",
    "    return y_pred"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.790597Z",
     "start_time": "2024-10-15T08:58:57.778638Z"
    }
   },
   "source": [
    "# Function to evaluate different kernels and select the best one\n",
    "def evaluate_kernels(X_train, y_train, X_test, y_test, kernels, tau=0.1):\n",
    "    mse_results = {}\n",
    "\n",
    "    for kernel_name, kernel_func in kernels.items():\n",
    "\n",
    "        y_pred = lwls_predict(X_train, y_train, X_test, kernel_func, tau=tau)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_results[kernel_name] = mse\n",
    "\n",
    "    return mse_results\n",
    "\n",
    "# Example kernels (ensure these are defined somewhere in the code)\n",
    "kernels = {\n",
    "    'Gaussian': gaussian,\n",
    "    'Epanechnikov': epanechnikov,\n",
    "    'Tricube': tricube,\n",
    "    'Uniform': uniform,\n",
    "    'Triangle': triangle\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.829667Z",
     "start_time": "2024-10-15T08:58:57.790597Z"
    }
   },
   "source": [
    "# Evaluate kernels on smaller data for faster results\n",
    "mse_results_small = evaluate_kernels(X_sample_train_with_bias, y_sample_train, X_sample_test_with_bias, y_sample_test, kernels)\n",
    "\n",
    "print(mse_results_small)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gaussian': 0.017111643456072034, 'Epanechnikov': 0.015838563409984314, 'Tricube': 0.015838563409984314, 'Uniform': 0.0009889360972607534, 'Triangle': 0.015838563409984314}\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:03:17.807222Z",
     "start_time": "2024-10-15T08:58:57.832185Z"
    }
   },
   "source": [
    "# Evaluation on the kernel selected\n",
    "def evaluate_uniform(X_train, y_train, X_test, y_test, uniform, tau=0.1):\n",
    "    for kernel_name, kernel_func in uniform.items():\n",
    "        y_pred = lwls_predict(X_train, y_train, X_test, kernel_func, tau=tau)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae_wls_uniform = mean_absolute_error(y_test, y_pred)\n",
    "        r2_wls_uniform = r2_score(y_test, y_pred)\n",
    "        rmse_wls_uniform = np.sqrt(mse)\n",
    "        \n",
    "        print(f\"{kernel_name} Kernel Results:\")\n",
    "        print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "        print(f\"Mean Absolute Error (MAE): {mae_wls_uniform}\")\n",
    "        print(f\"R-squared (R2): {r2_wls_uniform}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse_wls_uniform}\")\n",
    "\n",
    "    return\n",
    "\n",
    "uniform_kernel = {\n",
    "    'Uniform': uniform\n",
    "}\n",
    "\n",
    "evaluate_uniform(X_large_sample_with_bias, y_large_sample, X_large_test_sample_with_bias, y_large_test_sample, uniform_kernel)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform Kernel Results:\n",
      "Mean Squared Error (MSE): 0.0009096630799356888\n",
      "Mean Absolute Error (MAE): 0.022935803113481798\n",
      "R-squared (R2): 0.6475178661281868\n",
      "Root Mean Squared Error (RMSE): 0.030160621345318616\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 Regularization\n",
    "Ridge and Lasso regression are applied to test if the variance of the dataset can and has to be improved. Applying one of these techniques could make the model more stable and improve the prediction. \n",
    "Different alpha values are tested to see which one results in the best results. Both for Lasso and Ridge the goal is to minimize the mean squared error. To find the optimal alpha values GridSearchCV is used, which applies 5-fold cross-validation. "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:53:18.664693Z",
     "start_time": "2024-10-15T09:53:18.307228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a list with possible alpha values to iterate over\n",
    "alpha_values = {'alpha': [0.0001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Ridge model with cross-validation\n",
    "ridge_model = Ridge()\n",
    "# Apply GridSearchCV to search for the optimal alpha\n",
    "ridge_cv = GridSearchCV(ridge_model, param_grid=alpha_values, cv=5, scoring='neg_mean_squared_error') # cv=5 for 5-fold cross-validation, scoring mean squared error because the goal is to get this as low as possible\n",
    "ridge_cv.fit(X_large_test_sample,y_large_test_sample)\n",
    "\n",
    "# Lasso model with cross-validation\n",
    "lasso_model = Lasso()\n",
    "# Apply GridSearchCV to search for the optimal alpha\n",
    "lasso_cv = GridSearchCV(lasso_model, param_grid=alpha_values, cv=5, scoring='neg_mean_squared_error') # cv=5 for 5-fold cross-validation, scoring mean squared error because the goal is to get this as low as possible\n",
    "lasso_cv.fit(X_large_test_sample,y_large_test_sample)\n",
    "\n",
    "# Get the best alpha values\n",
    "best_alpha_ridge = ridge_cv.best_params_['alpha']\n",
    "best_alpha_lasso = lasso_cv.best_params_['alpha']\n",
    "\n",
    "print(f\"Optimal alpha for Ridge with the full dataset: {best_alpha_ridge}\")\n",
    "print(f\"Optimal alpha for Lasso with the full dataset: {best_alpha_lasso}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha for Ridge with the full dataset: 1\n",
      "Optimal alpha for Lasso with the full dataset: 0.0001\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:53:21.109609Z",
     "start_time": "2024-10-15T09:53:21.086511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the lasso model with the optimal alpha\n",
    "lasso_model = Lasso(alpha=best_alpha_lasso)\n",
    "lasso_model.fit(X_large_test_sample,y_large_test_sample)\n",
    "# Predict the power production with Lasso regularization\n",
    "y_pred_lasso = lasso_model.predict(X_large_test_sample)\n",
    "# Show the new coefficients\n",
    "lasso_model.coef_"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00056673, -0.03850683,  0.00719808,  0.0057766 ])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:53:48.292827Z",
     "start_time": "2024-10-15T09:53:48.276243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verify the model using the testing dataset and appropriate evaluation metrics\n",
    "mse_lasso = mean_squared_error(y_large_test_sample, y_pred_lasso)\n",
    "mae_lasso= mean_absolute_error(y_large_test_sample, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_large_test_sample, y_pred_lasso)\n",
    "rmse_lasso = np.sqrt(mse_lasso)\n",
    "\n",
    "print(f\"Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 0.0001:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_lasso:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_lasso:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_lasso:.4f}\")\n",
    "print(f\"R-squared: {r2_lasso:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 0.9:\n",
      "Root Mean Squared Error (RMSE): 0.0301\n",
      "Mean Squared Error (MSE): 0.0009\n",
      "Mean Absolute Error (MAE): 0.0229\n",
      "R-squared: 0.6491\n"
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:53:51.799766Z",
     "start_time": "2024-10-15T09:53:51.768849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the ridge model with the optimal alpha\n",
    "ridge_model = Ridge(alpha=best_alpha_ridge)\n",
    "ridge_model.fit(X_large_test_sample,y_large_test_sample)\n",
    "# Predict the power production with Ridge regularization\n",
    "y_pred_ridge = ridge_model.predict(X_large_test_sample)\n",
    "# Show the new coefficients\n",
    "ridge_model.coef_"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00066934, -0.03855133,  0.00735763,  0.00600741])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:53:55.162880Z",
     "start_time": "2024-10-15T09:53:55.143886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verify the model using the testing dataset and appropriate evaluation metrics\n",
    "mse_ridge = mean_squared_error(y_large_test_sample, y_pred_lasso)\n",
    "mae_ridge= mean_absolute_error(y_large_test_sample, y_pred_lasso)\n",
    "r2_ridge = r2_score(y_large_test_sample, y_pred_lasso)\n",
    "rmse_ridge = np.sqrt(mse_lasso)\n",
    "\n",
    "print(f\"Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 1:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_ridge:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_ridge:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_ridge:.4f}\")\n",
    "print(f\"R-squared: {r2_ridge:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Least Squares model evaluation on the testing dataset and the lasso regression with a penalty of 0.001:\n",
      "Root Mean Squared Error (RMSE): 0.0301\n",
      "Mean Squared Error (MSE): 0.0009\n",
      "Mean Absolute Error (MAE): 0.0229\n",
      "R-squared: 0.6491\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T11:19:50.442694Z",
     "start_time": "2024-10-15T11:19:50.400279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the ridge model with the optimal alpha\n",
    "ridge_model = Ridge(alpha=0.0001)\n",
    "ridge_model.fit(X_large_test_sample,y_large_test_sample)\n",
    "# Predict the power production with Ridge regularization\n",
    "y_pred_ridge = ridge_model.predict(X_large_test_sample)\n",
    "# Show the new coefficients\n",
    "ridge_model.coef_"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00067399, -0.03857578,  0.00736154,  0.00600847])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T11:19:59.896043Z",
     "start_time": "2024-10-15T11:19:59.860004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verify the model using the testing dataset and appropriate evaluation metrics\n",
    "mse_ridge = mean_squared_error(y_large_test_sample, y_pred_lasso)\n",
    "mae_ridge= mean_absolute_error(y_large_test_sample, y_pred_lasso)\n",
    "r2_ridge = r2_score(y_large_test_sample, y_pred_lasso)\n",
    "rmse_ridge = np.sqrt(mse_lasso)\n",
    "\n",
    "print(f\"Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 1:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_ridge:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_ridge:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_ridge:.4f}\")\n",
    "print(f\"R-squared: {r2_ridge:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 1:\n",
      "Root Mean Squared Error (RMSE): 0.0301\n",
      "Mean Squared Error (MSE): 0.0009\n",
      "Mean Absolute Error (MAE): 0.0229\n",
      "R-squared: 0.6491\n"
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The optimal penalty term for Lasso regression is 0.0001. This results in validation metric values with almost the same results as normal regression. Moreover, such a small penalty term indicates that it would be better to simply apply normal regression. This makes sense because for this model n>>p. There is a very large amount of data points and only 4 parameters. Consequently, there is already very low variance, before regularization is applied, minimizing the need for additional regularization. \n",
    "\n",
    "Ridge regression shows similar results, with one notable difference. For Ridge regression, it does not matter whether the penalty term is set to 1 or to 0.0001. The validation metrics remain exactly the same. This indicates that the data is inherently regularized; there is already a very low variance and the Ridge regression is not required to improve results. \n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
