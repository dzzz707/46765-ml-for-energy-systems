{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T10:28:31.210393Z",
     "start_time": "2024-10-15T10:28:31.199915Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T10:28:33.591292Z",
     "start_time": "2024-10-15T10:28:33.526825Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = 'Data assignment 1/Feature data.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step the different features are scaled, in order to make sure the models can interpret the features on a similar scale. The wind speed and the maximum temperature undergo the standardscaler, while wind direction is converted to sinus and cosinus components. The power production is normalized using the nominal capacity of 30 MW (https://stateofgreen.com/en/solutions/kalby-wind-turbines/). \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T10:28:36.353421Z",
     "start_time": "2024-10-15T10:28:36.304077Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import required scalers\n",
    "scaler_standard = StandardScaler()\n",
    "\n",
    "### 1. Standard Scaling for wind speed and temperature\n",
    "data['Mean wind speed'] = scaler_standard.fit_transform(data[['Mean wind speed']])\n",
    "data['Maximum temperature'] = scaler_standard.fit_transform(data[['Maximum temperature']])\n",
    "\n",
    "### 2. Wind Direction (convert to sin and cos components)\n",
    "data['Wind direction sin'] = np.sin(np.deg2rad(data['Mean wind direction']))\n",
    "data['Wind direction cos'] = np.cos(np.deg2rad(data['Mean wind direction']))\n",
    "\n",
    "### 3. Normalize Power Production \n",
    "nominal_capacity = 30000 # production capacity is 30 MW, unit of power production is kW so nominal capacity is 30000 (kW)\n",
    "data['AKI Kalby Active Power'] = data['AKI Kalby Active Power'] / nominal_capacity\n",
    "\n",
    "# Dropping the original wind direction after scaling\n",
    "data = data.drop('Mean wind direction', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T10:28:38.642843Z",
     "start_time": "2024-10-15T10:28:38.598070Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make sure datetime is set as the index\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "data.set_index('datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T10:28:39.016577Z",
     "start_time": "2024-10-15T10:28:39.007306Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set target and features, and remove non-numeric columns\n",
    "target_column = 'AKI Kalby Active Power'\n",
    "features = data.select_dtypes(include=[np.number]).drop(columns=[target_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T10:28:39.948455Z",
     "start_time": "2024-10-15T10:28:39.932134Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AKI Kalby Active Power</th>\n",
       "      <th>Maximum temperature</th>\n",
       "      <th>Mean wind speed</th>\n",
       "      <th>Wind direction sin</th>\n",
       "      <th>Wind direction cos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:00:00</th>\n",
       "      <td>-0.063118</td>\n",
       "      <td>-0.457945</td>\n",
       "      <td>0.868655</td>\n",
       "      <td>-0.998630</td>\n",
       "      <td>-5.233596e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 01:00:00</th>\n",
       "      <td>-0.055728</td>\n",
       "      <td>-0.457945</td>\n",
       "      <td>0.382418</td>\n",
       "      <td>-0.956305</td>\n",
       "      <td>-2.923717e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 02:00:00</th>\n",
       "      <td>-0.095724</td>\n",
       "      <td>-0.503187</td>\n",
       "      <td>0.756447</td>\n",
       "      <td>-0.994522</td>\n",
       "      <td>-1.045285e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 03:00:00</th>\n",
       "      <td>-0.063726</td>\n",
       "      <td>-0.518268</td>\n",
       "      <td>0.494627</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 04:00:00</th>\n",
       "      <td>-0.029392</td>\n",
       "      <td>-0.473025</td>\n",
       "      <td>0.307612</td>\n",
       "      <td>-0.951057</td>\n",
       "      <td>3.090170e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     AKI Kalby Active Power  Maximum temperature  \\\n",
       "datetime                                                           \n",
       "2022-01-01 00:00:00               -0.063118            -0.457945   \n",
       "2022-01-01 01:00:00               -0.055728            -0.457945   \n",
       "2022-01-01 02:00:00               -0.095724            -0.503187   \n",
       "2022-01-01 03:00:00               -0.063726            -0.518268   \n",
       "2022-01-01 04:00:00               -0.029392            -0.473025   \n",
       "\n",
       "                     Mean wind speed  Wind direction sin  Wind direction cos  \n",
       "datetime                                                                      \n",
       "2022-01-01 00:00:00         0.868655           -0.998630       -5.233596e-02  \n",
       "2022-01-01 01:00:00         0.382418           -0.956305       -2.923717e-01  \n",
       "2022-01-01 02:00:00         0.756447           -0.994522       -1.045285e-01  \n",
       "2022-01-01 03:00:00         0.494627           -1.000000       -1.836970e-16  \n",
       "2022-01-01 04:00:00         0.307612           -0.951057        3.090170e-01  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataframe to check if scaling worked\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing testing and training \n",
    "Using train_test_split the data is split into testing and training data. The choice was made to not use TimeSeriesSplit, because although the data is time based, the values are not dependent on the time of day in the sense that there is no strong temporal relationship that affects the observations. The power production is more weather dependent than anything else. \n",
    " The data points can be treated independently of their time indices, allowing for a standard random sampling approach. By maintaining a randomized split, we also prevent potential biases that could arise from time-based sequences, ensuring that both the training and testing set represent the overall distribution of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:55.754383Z",
     "start_time": "2024-10-15T08:58:55.731495Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "# Select a 100 datapoints as a start with a low number of samples\n",
    "X_sample = features[:100]\n",
    "y_sample = data[target_column][:100]\n",
    "\n",
    "# Sequential split (shuffle=False)\n",
    "# Give the random_state a set seed of 42, to ensure that the split will be the same everytime in order to reproduce results\n",
    "X_sample_train, X_sample_test, y_sample_train, y_sample_test = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.2, shuffle=False, random_state=42)\n",
    "\n",
    "# Adding a column of ones to X_sample for the bias term and converting to NumPy array\n",
    "X_sample_train_with_bias = np.c_[np.ones(X_sample_train.shape[0]), X_sample_train].astype(float)\n",
    "X_sample_test_with_bias = np.c_[np.ones(X_sample_test.shape[0]), X_sample_test].astype(float)\n",
    "\n",
    "# Ensure y_sample_train is also a NumPy array\n",
    "y_sample_train = np.array(y_sample_train).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.233572Z",
     "start_time": "2024-10-15T08:58:55.778270Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gradient Descent function\n",
    "def gradient_descent(X, y, learning_rate=0.01, epochs=100000):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    for _ in range(epochs):\n",
    "        y_pred = X @ theta\n",
    "        gradients = (1/m) * X.T @ (y_pred - y)\n",
    "        theta -= learning_rate * gradients\n",
    "    return theta\n",
    "\n",
    "theta_gd = gradient_descent(X_sample_train_with_bias, y_sample_train)\n",
    "\n",
    "# Predictions using Gradient Descent\n",
    "y_pred_gd = X_sample_test_with_bias @ theta_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.534123Z",
     "start_time": "2024-10-15T08:58:57.233572Z"
    }
   },
   "outputs": [],
   "source": [
    "# Closed-form solution \n",
    "theta_closed_form = np.linalg.inv(X_sample_train_with_bias.T @ X_sample_train_with_bias) @ X_sample_train_with_bias.T @ y_sample_train\n",
    "\n",
    "# Predictions using closed-form solution\n",
    "y_pred_closed_form = X_sample_test_with_bias @ theta_closed_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.547467Z",
     "start_time": "2024-10-15T08:58:57.534123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent θ: ['-0.03789', '0.02592', '-0.04119', '0.01152', '0.01116']\n",
      "Closed-Form θ: ['-0.03789', '0.02592', '-0.04119', '0.01152', '0.01116']\n",
      "Gradient Descent MSE: 0.00099\n",
      "Closed-Form MSE: 0.00099\n"
     ]
    }
   ],
   "source": [
    "# mse calculation\n",
    "mse_gd = mean_squared_error(y_sample_test, y_pred_gd)\n",
    "mse_closed_form = mean_squared_error(y_sample_test, y_pred_closed_form)\n",
    "\n",
    "print(f\"Gradient Descent θ: {[f'{x:.5f}' for x in theta_gd]}\")\n",
    "print(f\"Closed-Form θ: {[f'{x:.5f}' for x in theta_closed_form]}\")\n",
    "print(f\"Gradient Descent MSE: {mse_gd:.5f}\")\n",
    "print(f\"Closed-Form MSE: {mse_closed_form:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.580788Z",
     "start_time": "2024-10-15T08:58:57.547467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3.2: Closed-form solution training complete on the larger sample.\n",
      "Coefficients: ['-0.04394', '0.00016', '-0.03909', '0.00591', '0.00366']\n"
     ]
    }
   ],
   "source": [
    "# Step 3.2: Use the full dataset and closed form solution\n",
    "X_large_sample, X_large_test_sample, y_large_sample, y_large_test_sample = train_test_split(features, data[target_column], test_size=0.2, random_state=42)\n",
    "\n",
    "# Adding a column of ones for the bias term in the large sample\n",
    "X_large_sample_with_bias = np.c_[np.ones(X_large_sample.shape[0]), X_large_sample]\n",
    "X_large_test_sample_with_bias = np.c_[np.ones(X_large_test_sample.shape[0]), X_large_test_sample]\n",
    "\n",
    "# Upgrade the normal equation\n",
    "theta_large_sample = np.linalg.inv(X_large_sample_with_bias.T @ X_large_sample_with_bias) @ X_large_sample_with_bias.T @ y_large_sample\n",
    "theta_large_sample_rounded = np.round(theta_large_sample, 5)\n",
    "\n",
    "print(f\"Step 3.2: Closed-form solution training complete on the larger sample.\")\n",
    "print(f\"Coefficients: {[f'{x:.5f}' for x in theta_large_sample_rounded]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.604789Z",
     "start_time": "2024-10-15T08:58:57.580788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3.3: Model evaluation on the testing dataset:\n",
      "Root Mean Squared Error (RMSE): 0.03016\n",
      "Mean Squared Error (MSE): 0.00091\n",
      "Mean Absolute Error (MAE): 0.02294\n",
      "R-squared: 0.64752\n"
     ]
    }
   ],
   "source": [
    "# Step 3.3: Verify your model using the testing dataset and appropriate evaluation metrics\n",
    "y_large_pred_closed_form = X_large_test_sample_with_bias @ theta_large_sample\n",
    "\n",
    "mse = mean_squared_error(y_large_test_sample, y_large_pred_closed_form)\n",
    "mae = mean_absolute_error(y_large_test_sample, y_large_pred_closed_form)\n",
    "r2 = r2_score(y_large_test_sample, y_large_pred_closed_form)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Step 3.3: Model evaluation on the testing dataset:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.5f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.5f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.5f}\")\n",
    "print(f\"R-squared: {r2:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Non-linear Regression\n",
    "\n",
    "In Step 1's formulation, if the price $\\lambda$ is treated as a constant and the actual value p is known, the entire formula simplifies into a function dependent on the predicted value $\\hat{p}_t$. This implies that the problem can be reframed as an optimization task concerning the prediction of $\\hat{p}_t$. Given this perspective, extending the linear regression model from Step 3 by incorporating nonlinear features to predict $\\hat{p}_t$ effectively transforms the problem into a nonlinear regression for Step 1's objective. Therefore, performing nonlinear regression on the prediction model of $\\hat{p}_t$ inherently satisfies the requirements of the nonlinear extension outlined in Step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.627386Z",
     "start_time": "2024-10-15T08:58:57.604789Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross-validation function, which returns RMSE\n",
    "def perform_cross_validation(X, y, degree, n_splits=10):\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    # Initialize linear regression model\n",
    "    linear_model = LinearRegression()\n",
    "    \n",
    "    # Use KFold for cross-validation, n_splits set to 10\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Compute RMSE via cross-validation (use negative MSE and then take the square root)\n",
    "    neg_mse_scores = cross_val_score(linear_model, X_poly, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    # Convert negative MSE to RMSE\n",
    "    rmse_scores = np.sqrt(-neg_mse_scores)\n",
    "    \n",
    "    # Return the mean RMSE\n",
    "    return rmse_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating degree 2 polynomial: \n",
      "Mean RMSE for degree 2: 0.02963\n",
      "Evaluating degree 3 polynomial: \n",
      "Mean RMSE for degree 3: 0.02762\n",
      "Evaluating degree 4 polynomial: \n",
      "Mean RMSE for degree 4: 0.02725\n"
     ]
    }
   ],
   "source": [
    "# Polynomial degrees to evaluate: 2, 3, and 4\n",
    "degrees = [2, 3, 4]\n",
    "best_degree = None\n",
    "best_rmse = float('inf')\n",
    "\n",
    "# Perform cross-validation for each polynomial degree\n",
    "for degree in degrees:\n",
    "    print(f\"Evaluating degree {degree} polynomial: \")\n",
    "    rmse = perform_cross_validation(features, data[target_column], degree)\n",
    "    print(f\"Mean RMSE for degree {degree}: {rmse:.5f}\")\n",
    "    \n",
    "    # Update the best model based on RMSE\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_degree = degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best degree is 4 with RMSE: 0.02725\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best degree is {best_degree} with RMSE: {best_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.731195Z",
     "start_time": "2024-10-15T08:58:57.627386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE for the best degree 4: 0.02752\n",
      "Test MAE for the best degree 4: 0.02016\n",
      "Test R-squared for the best degree 4: 0.70646\n"
     ]
    }
   ],
   "source": [
    "# Generate the best polynomial features\n",
    "poly_best = PolynomialFeatures(degree=best_degree, include_bias=False)\n",
    "X_poly_best = poly_best.fit_transform(features)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_poly_best, X_test_poly_best, y_train_poly_best, y_test_poly_best = train_test_split(\n",
    "    X_poly_best, data[target_column], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the final polynomial model\n",
    "linear_model_best = LinearRegression()\n",
    "linear_model_best.fit(X_train_poly_best, y_train_poly_best)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_poly_best = linear_model_best.predict(X_test_poly_best)\n",
    "\n",
    "test_rmse_best = np.sqrt(mean_squared_error(y_test_poly_best, y_pred_poly_best))\n",
    "test_mae_best = mean_absolute_error(y_test_poly_best, y_pred_poly_best)\n",
    "test_r2_best = r2_score(y_test_poly_best, y_pred_poly_best)\n",
    "\n",
    "# Evaluation\n",
    "print(f\"Test RMSE for the best degree {best_degree}: {test_rmse_best:.5f}\")\n",
    "print(f\"Test MAE for the best degree {best_degree}: {test_mae_best:.5f}\")\n",
    "print(f\"Test R-squared for the best degree {best_degree}: {test_r2_best:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For step 4.2, the method of locally weighted least squares will be used, as tought in the lecture. Different kernels will be compared and the best one will be chosen based on evaluating the performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.765994Z",
     "start_time": "2024-10-15T08:58:57.747447Z"
    }
   },
   "outputs": [],
   "source": [
    "def gaussian(t):\n",
    "    return np.exp(-0.5 * t**2) / np.sqrt(2 * np.pi)\n",
    "\n",
    "def epanechnikov(t):\n",
    "    res = np.zeros_like(t)\n",
    "    res[np.abs(t) <= 1] = 0.75 * (1 - t[np.abs(t) <= 1]**2)\n",
    "    return res\n",
    "\n",
    "def tricube(t):\n",
    "    res = np.zeros_like(t)\n",
    "    res[np.abs(t) <= 1] = (70 / 81) * (1 - np.abs(t[np.abs(t) <= 1])**3)**3\n",
    "    return res\n",
    "\n",
    "def uniform(t, p=0.2):\n",
    "    return np.zeros_like(t) + p\n",
    "\n",
    "def triangle(t):\n",
    "    res = np.zeros_like(t)\n",
    "    res[np.abs(t) <= 1] = 1 - np.abs(t[np.abs(t) <= 1])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.778638Z",
     "start_time": "2024-10-15T08:58:57.769007Z"
    }
   },
   "outputs": [],
   "source": [
    "# Locally Weighted Least Squares implementation\n",
    "def lwls_predict(X_train, y_train, X_test, kernel_func, tau=0.1):\n",
    "    y_pred = np.zeros(len(X_test))\n",
    "\n",
    "    for i, x in enumerate(X_test):\n",
    "        distances = np.linalg.norm(X_train - x, axis=1)  # Compute distances\n",
    "        weights = kernel_func(distances / tau)  # Apply kernel function\n",
    "        W = np.diag(weights)  # Create diagonal weight matrix\n",
    "\n",
    "        # Weighted Least Squares computation\n",
    "        XTWX = X_train.T @ W @ X_train  # X^T W X\n",
    "        XTWy = X_train.T @ W @ y_train  # X^T W y\n",
    "\n",
    "        # Use np.linalg.pinv for numerical stability\n",
    "        theta = np.linalg.pinv(XTWX) @ XTWy\n",
    "\n",
    "        # Ensure x is 2D before matrix multiplication\n",
    "        y_pred[i] = np.dot(x, theta)  # Prediction for the current test sample\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.790597Z",
     "start_time": "2024-10-15T08:58:57.778638Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to evaluate different kernels and select the best one\n",
    "def evaluate_kernels(X_train, y_train, X_test, y_test, kernels, tau=0.1):\n",
    "    mse_results = {}\n",
    "\n",
    "    for kernel_name, kernel_func in kernels.items():\n",
    "\n",
    "        y_pred = lwls_predict(X_train, y_train, X_test, kernel_func, tau=tau)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_results[kernel_name] = mse\n",
    "\n",
    "    return mse_results\n",
    "\n",
    "# Example kernels (ensure these are defined somewhere in the code)\n",
    "kernels = {\n",
    "    'Gaussian': gaussian,\n",
    "    'Epanechnikov': epanechnikov,\n",
    "    'Tricube': tricube,\n",
    "    'Uniform': uniform,\n",
    "    'Triangle': triangle\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:58:57.829667Z",
     "start_time": "2024-10-15T08:58:57.790597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gaussian': 0.017111643456072034, 'Epanechnikov': 0.015838563409984314, 'Tricube': 0.015838563409984314, 'Uniform': 0.0009889360972607534, 'Triangle': 0.015838563409984314}\n",
      "The kernel with the smallest MSE is 'Uniform' with a value of 0.0009889360972607534\n"
     ]
    }
   ],
   "source": [
    "# Evaluate kernels on smaller data for faster results\n",
    "mse_results_small = evaluate_kernels(X_sample_train_with_bias, y_sample_train, X_sample_test_with_bias, y_sample_test, kernels)\n",
    "min_kernel, min_mse = min(mse_results_small.items(), key=lambda x: x[1])\n",
    "\n",
    "# Print the full results and the minimum one\n",
    "print(mse_results_small)\n",
    "print(f\"The kernel with the smallest MSE is '{min_kernel}' with a value of {min_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:03:17.807222Z",
     "start_time": "2024-10-15T08:58:57.832185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform Kernel Results:\n",
      "Mean Squared Error (MSE): 0.0009096630799356888\n",
      "Mean Absolute Error (MAE): 0.0229358031134818\n",
      "R-squared (R2): 0.6475178661281868\n",
      "Root Mean Squared Error (RMSE): 0.030160621345318616\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the kernel selected\n",
    "def evaluate_uniform(X_train, y_train, X_test, y_test, uniform, tau=0.1):\n",
    "    for kernel_name, kernel_func in uniform.items():\n",
    "        y_pred = lwls_predict(X_train, y_train, X_test, kernel_func, tau=tau)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae_wls_uniform = mean_absolute_error(y_test, y_pred)\n",
    "        r2_wls_uniform = r2_score(y_test, y_pred)\n",
    "        rmse_wls_uniform = np.sqrt(mse)\n",
    "        \n",
    "        print(f\"{kernel_name} Kernel Results:\")\n",
    "        print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "        print(f\"Mean Absolute Error (MAE): {mae_wls_uniform}\")\n",
    "        print(f\"R-squared (R2): {r2_wls_uniform}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse_wls_uniform}\")\n",
    "\n",
    "    return\n",
    "\n",
    "uniform_kernel = {\n",
    "    'Uniform': uniform\n",
    "}\n",
    "\n",
    "evaluate_uniform(X_large_sample_with_bias, y_large_sample, X_large_test_sample_with_bias, y_large_test_sample, uniform_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 Regularization\n",
    "Ridge and Lasso regression are applied to test if the variance of the dataset can and has to be improved. Applying one of these techniques could make the model more stable and improve the prediction. \n",
    "Different alpha values are tested to see which one results in the best results. Both for Lasso and Ridge the goal is to minimize the mean squared error. To find the optimal alpha values GridSearchCV is used, which applies 5-fold cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:53:18.664693Z",
     "start_time": "2024-10-15T09:53:18.307228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha for Ridge with the full dataset: 1\n",
      "Optimal alpha for Lasso with the full dataset: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Create a list with possible alpha values to iterate over\n",
    "alpha_values = {'alpha': [0.0001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Ridge model with cross-validation\n",
    "ridge_model = Ridge()\n",
    "# Apply GridSearchCV to search for the optimal alpha\n",
    "ridge_cv = GridSearchCV(ridge_model, param_grid=alpha_values, cv=5, scoring='neg_mean_squared_error') # cv=5 for 5-fold cross-validation, scoring mean squared error because the goal is to get this as low as possible\n",
    "ridge_cv.fit(X_large_test_sample,y_large_test_sample)\n",
    "\n",
    "# Lasso model with cross-validation\n",
    "lasso_model = Lasso()\n",
    "# Apply GridSearchCV to search for the optimal alpha\n",
    "lasso_cv = GridSearchCV(lasso_model, param_grid=alpha_values, cv=5, scoring='neg_mean_squared_error') # cv=5 for 5-fold cross-validation, scoring mean squared error because the goal is to get this as low as possible\n",
    "lasso_cv.fit(X_large_test_sample,y_large_test_sample)\n",
    "\n",
    "# Get the best alpha values\n",
    "best_alpha_ridge = ridge_cv.best_params_['alpha']\n",
    "best_alpha_lasso = lasso_cv.best_params_['alpha']\n",
    "\n",
    "print(f\"Optimal alpha for Ridge with the full dataset: {best_alpha_ridge}\")\n",
    "print(f\"Optimal alpha for Lasso with the full dataset: {best_alpha_lasso}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:53:21.109609Z",
     "start_time": "2024-10-15T09:53:21.086511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00056673, -0.03850683,  0.00719808,  0.0057766 ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the lasso model with the optimal alpha\n",
    "lasso_model = Lasso(alpha=best_alpha_lasso)\n",
    "lasso_model.fit(X_large_test_sample,y_large_test_sample)\n",
    "# Predict the power production with Lasso regularization\n",
    "y_pred_lasso = lasso_model.predict(X_large_test_sample)\n",
    "# Show the new coefficients\n",
    "lasso_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:53:48.292827Z",
     "start_time": "2024-10-15T09:53:48.276243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 0.0001:\n",
      "Root Mean Squared Error (RMSE): 0.0301\n",
      "Mean Squared Error (MSE): 0.0009\n",
      "Mean Absolute Error (MAE): 0.0229\n",
      "R-squared: 0.6491\n"
     ]
    }
   ],
   "source": [
    "# Verify the model using the testing dataset and appropriate evaluation metrics\n",
    "mse_lasso = mean_squared_error(y_large_test_sample, y_pred_lasso)\n",
    "mae_lasso= mean_absolute_error(y_large_test_sample, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_large_test_sample, y_pred_lasso)\n",
    "rmse_lasso = np.sqrt(mse_lasso)\n",
    "\n",
    "print(f\"Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 0.0001:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_lasso:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_lasso:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_lasso:.4f}\")\n",
    "print(f\"R-squared: {r2_lasso:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:53:51.799766Z",
     "start_time": "2024-10-15T09:53:51.768849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00066934, -0.03855133,  0.00735763,  0.00600741])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the ridge model with the optimal alpha\n",
    "ridge_model = Ridge(alpha=best_alpha_ridge)\n",
    "ridge_model.fit(X_large_test_sample,y_large_test_sample)\n",
    "# Predict the power production with Ridge regularization\n",
    "y_pred_ridge = ridge_model.predict(X_large_test_sample)\n",
    "# Show the new coefficients\n",
    "ridge_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:53:55.162880Z",
     "start_time": "2024-10-15T09:53:55.143886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 1:\n",
      "Root Mean Squared Error (RMSE): 0.0301\n",
      "Mean Squared Error (MSE): 0.0009\n",
      "Mean Absolute Error (MAE): 0.0229\n",
      "R-squared: 0.6491\n"
     ]
    }
   ],
   "source": [
    "# Verify the model using the testing dataset and appropriate evaluation metrics\n",
    "mse_ridge = mean_squared_error(y_large_test_sample, y_pred_lasso)\n",
    "mae_ridge= mean_absolute_error(y_large_test_sample, y_pred_lasso)\n",
    "r2_ridge = r2_score(y_large_test_sample, y_pred_lasso)\n",
    "rmse_ridge = np.sqrt(mse_lasso)\n",
    "\n",
    "print(f\"Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 1:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_ridge:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_ridge:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_ridge:.4f}\")\n",
    "print(f\"R-squared: {r2_ridge:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T11:19:50.442694Z",
     "start_time": "2024-10-15T11:19:50.400279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00067399, -0.03857578,  0.00736154,  0.00600847])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the ridge model with the optimal alpha\n",
    "ridge_model = Ridge(alpha=0.0001)\n",
    "ridge_model.fit(X_large_test_sample,y_large_test_sample)\n",
    "# Predict the power production with Ridge regularization\n",
    "y_pred_ridge = ridge_model.predict(X_large_test_sample)\n",
    "# Show the new coefficients\n",
    "ridge_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T11:19:59.896043Z",
     "start_time": "2024-10-15T11:19:59.860004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 1:\n",
      "Root Mean Squared Error (RMSE): 0.0301\n",
      "Mean Squared Error (MSE): 0.0009\n",
      "Mean Absolute Error (MAE): 0.0229\n",
      "R-squared: 0.6491\n"
     ]
    }
   ],
   "source": [
    "# Verify the model using the testing dataset and appropriate evaluation metrics\n",
    "mse_ridge = mean_squared_error(y_large_test_sample, y_pred_lasso)\n",
    "mae_ridge= mean_absolute_error(y_large_test_sample, y_pred_lasso)\n",
    "r2_ridge = r2_score(y_large_test_sample, y_pred_lasso)\n",
    "rmse_ridge = np.sqrt(mse_lasso)\n",
    "\n",
    "print(f\"Weighted Least Squares model evaluation on the testing dataset and the ridge regression with a penalty of 1:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_ridge:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_ridge:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_ridge:.4f}\")\n",
    "print(f\"R-squared: {r2_ridge:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal penalty term for Lasso regression is 0.0001. This results in validation metric values with almost the same results as normal regression. Moreover, such a small penalty term indicates that it would be better to simply apply normal regression. This makes sense because for this model n>>p. There is a very large amount of data points and only 4 parameters. Consequently, there is already very low variance, before regularization is applied, minimizing the need for additional regularization. \n",
    "\n",
    "Ridge regression shows similar results, with one notable difference. For Ridge regression, it does not matter whether the penalty term is set to 1 or to 0.0001. The validation metrics remain exactly the same. This indicates that the data is inherently regularized; there is already a very low variance and the Ridge regression is not required to improve results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
