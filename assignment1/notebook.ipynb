{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:03:56.489232Z",
     "start_time": "2024-10-07T15:03:56.484579Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:20:24.078251Z",
     "start_time": "2024-10-07T14:20:24.046208Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = 'Data assignment 1/Feature data.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step,we used Principal Component Analysis (PCA) to select and reduce the dimensionality of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:20:26.705850Z",
     "start_time": "2024-10-07T14:20:26.676852Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler_standard = StandardScaler()\n",
    "scaler_minmax = MinMaxScaler()\n",
    "\n",
    "### 1. Standard Scaling for wind speed and temperature\n",
    "data['Mean wind speed'] = scaler_standard.fit_transform(data[['Mean wind speed']])\n",
    "data['Maximum temperature'] = scaler_standard.fit_transform(data[['Maximum temperature']])\n",
    "\n",
    "### 2. Wind Direction (convert to sin and cos components)\n",
    "data['Wind direction sin'] = np.sin(np.deg2rad(data['Mean wind direction']))\n",
    "data['Wind direction cos'] = np.cos(np.deg2rad(data['Mean wind direction']))\n",
    "\n",
    "### 3. Normalize Power Production \n",
    "nominal_capacity = 30000 # production capacity is 30 MW, unit of power production is kW so nominal capacity is 30000\n",
    "data['AKI Kalby Active Power'] = data['AKI Kalby Active Power'] / nominal_capacity\n",
    "\n",
    "# Dropping the original wind direction after transformation\n",
    "data = data.drop('Mean wind direction', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure datetime is set as the index\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "data.set_index('datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:20:28.968566Z",
     "start_time": "2024-10-07T14:20:28.962759Z"
    }
   },
   "outputs": [],
   "source": [
    "# set target and features, and remove non-numeric columns\n",
    "target_column = 'AKI Kalby Active Power'\n",
    "features = data.select_dtypes(include=[np.number]).drop(columns=[target_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T16:43:54.020265Z",
     "start_time": "2024-10-05T16:43:53.943334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AKI Kalby Active Power</th>\n",
       "      <th>Maximum temperature</th>\n",
       "      <th>Mean wind speed</th>\n",
       "      <th>Wind direction sin</th>\n",
       "      <th>Wind direction cos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:00:00</th>\n",
       "      <td>-0.063118</td>\n",
       "      <td>-0.457945</td>\n",
       "      <td>0.868655</td>\n",
       "      <td>-0.998630</td>\n",
       "      <td>-5.233596e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 01:00:00</th>\n",
       "      <td>-0.055728</td>\n",
       "      <td>-0.457945</td>\n",
       "      <td>0.382418</td>\n",
       "      <td>-0.956305</td>\n",
       "      <td>-2.923717e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 02:00:00</th>\n",
       "      <td>-0.095724</td>\n",
       "      <td>-0.503187</td>\n",
       "      <td>0.756447</td>\n",
       "      <td>-0.994522</td>\n",
       "      <td>-1.045285e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 03:00:00</th>\n",
       "      <td>-0.063726</td>\n",
       "      <td>-0.518268</td>\n",
       "      <td>0.494627</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 04:00:00</th>\n",
       "      <td>-0.029392</td>\n",
       "      <td>-0.473025</td>\n",
       "      <td>0.307612</td>\n",
       "      <td>-0.951057</td>\n",
       "      <td>3.090170e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 19:00:00</th>\n",
       "      <td>-0.148665</td>\n",
       "      <td>0.009559</td>\n",
       "      <td>1.953338</td>\n",
       "      <td>-0.656059</td>\n",
       "      <td>-7.547096e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 20:00:00</th>\n",
       "      <td>-0.153192</td>\n",
       "      <td>0.039721</td>\n",
       "      <td>1.467101</td>\n",
       "      <td>-0.731354</td>\n",
       "      <td>-6.819984e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 21:00:00</th>\n",
       "      <td>-0.120257</td>\n",
       "      <td>0.039721</td>\n",
       "      <td>1.504504</td>\n",
       "      <td>-0.681998</td>\n",
       "      <td>-7.313537e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 22:00:00</th>\n",
       "      <td>-0.103334</td>\n",
       "      <td>-0.065844</td>\n",
       "      <td>1.242684</td>\n",
       "      <td>-0.798636</td>\n",
       "      <td>-6.018150e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 23:00:00</th>\n",
       "      <td>-0.117154</td>\n",
       "      <td>-0.065844</td>\n",
       "      <td>0.868655</td>\n",
       "      <td>-0.743145</td>\n",
       "      <td>-6.691306e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7813 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     AKI Kalby Active Power  Maximum temperature  \\\n",
       "datetime                                                           \n",
       "2022-01-01 00:00:00               -0.063118            -0.457945   \n",
       "2022-01-01 01:00:00               -0.055728            -0.457945   \n",
       "2022-01-01 02:00:00               -0.095724            -0.503187   \n",
       "2022-01-01 03:00:00               -0.063726            -0.518268   \n",
       "2022-01-01 04:00:00               -0.029392            -0.473025   \n",
       "...                                     ...                  ...   \n",
       "2022-12-31 19:00:00               -0.148665             0.009559   \n",
       "2022-12-31 20:00:00               -0.153192             0.039721   \n",
       "2022-12-31 21:00:00               -0.120257             0.039721   \n",
       "2022-12-31 22:00:00               -0.103334            -0.065844   \n",
       "2022-12-31 23:00:00               -0.117154            -0.065844   \n",
       "\n",
       "                     Mean wind speed  Wind direction sin  Wind direction cos  \n",
       "datetime                                                                      \n",
       "2022-01-01 00:00:00         0.868655           -0.998630       -5.233596e-02  \n",
       "2022-01-01 01:00:00         0.382418           -0.956305       -2.923717e-01  \n",
       "2022-01-01 02:00:00         0.756447           -0.994522       -1.045285e-01  \n",
       "2022-01-01 03:00:00         0.494627           -1.000000       -1.836970e-16  \n",
       "2022-01-01 04:00:00         0.307612           -0.951057        3.090170e-01  \n",
       "...                              ...                 ...                 ...  \n",
       "2022-12-31 19:00:00         1.953338           -0.656059       -7.547096e-01  \n",
       "2022-12-31 20:00:00         1.467101           -0.731354       -6.819984e-01  \n",
       "2022-12-31 21:00:00         1.504504           -0.681998       -7.313537e-01  \n",
       "2022-12-31 22:00:00         1.242684           -0.798636       -6.018150e-01  \n",
       "2022-12-31 23:00:00         0.868655           -0.743145       -6.691306e-01  \n",
       "\n",
       "[7813 rows x 5 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:20:44.993Z",
     "start_time": "2024-10-07T14:20:44.979734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN indices: [   0    1    2 ... 6248 6249 6250] TEST indices: [6251 6252 6253 ... 7810 7811 7812]\n",
      "X_train shape: (6251, 4)\n",
      "X_test shape: (1562, 4)\n",
      "y_train shape: (6251,)\n",
      "y_test shape: (1562,)\n"
     ]
    }
   ],
   "source": [
    "# split the data\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "split_data= TimeSeriesSplit(n_splits=3, test_size=int(0.2*len(data)))\n",
    "for train_index, test_index in split_data.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "print(\"TRAIN indices:\", train_index, \"TEST indices:\", test_index)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:26:29.129022Z",
     "start_time": "2024-10-07T14:26:29.119042Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3.1 Please show that these two methods end up with the same solution.\n",
    "X_sample = features[:100]\n",
    "y_sample = data[target_column][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential split (shuffle=False)\n",
    "X_sample_train, X_sample_test, y_sample_train, y_sample_test = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.2, shuffle=False, random_state=42)\n",
    "\n",
    "# Adding a column of ones to X_sample for the bias term and converting to NumPy array\n",
    "X_sample_train_with_bias = np.c_[np.ones(X_sample_train.shape[0]), X_sample_train].astype(float)\n",
    "X_sample_test_with_bias = np.c_[np.ones(X_sample_test.shape[0]), X_sample_test].astype(float)\n",
    "\n",
    "# Ensure y_sample_train is also a NumPy array\n",
    "y_sample_train = np.array(y_sample_train).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:26:41.034898Z",
     "start_time": "2024-10-07T14:26:31.807318Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gradient Descent function\n",
    "def gradient_descent(X, y, learning_rate=0.01, epochs=100000):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    for _ in range(epochs):\n",
    "        y_pred = X @ theta\n",
    "        gradients = (1/m) * X.T @ (y_pred - y)\n",
    "        theta -= learning_rate * gradients\n",
    "    return theta\n",
    "\n",
    "# Run Gradient Descent\n",
    "theta_gd = gradient_descent(X_sample_train_with_bias, y_sample_train)\n",
    "\n",
    "# Predictions using Gradient Descent\n",
    "y_pred_gd = X_sample_test_with_bias @ theta_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:29:23.213388Z",
     "start_time": "2024-10-07T14:29:23.108653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Closed-form solution (Normal Equation)\n",
    "theta_closed_form = np.linalg.inv(X_sample_train_with_bias.T @ X_sample_train_with_bias) @ X_sample_train_with_bias.T @ y_sample_train\n",
    "\n",
    "# Predictions using closed-form solution\n",
    "y_pred_closed_form = X_sample_test_with_bias @ theta_closed_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:29:26.053054Z",
     "start_time": "2024-10-07T14:29:25.472272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Î¸: ['-0.03789', '0.02592', '-0.04119', '0.01152', '0.01116']\n",
      "Closed-Form Î¸: ['-0.03789', '0.02592', '-0.04119', '0.01152', '0.01116']\n",
      "Gradient Descent MSE: 0.00099\n",
      "Closed-Form MSE: 0.00099\n"
     ]
    }
   ],
   "source": [
    "# Mean Squared Error calculation\n",
    "mse_gd = mean_squared_error(y_sample_test, y_pred_gd)\n",
    "mse_closed_form = mean_squared_error(y_sample_test, y_pred_closed_form)\n",
    "\n",
    "# Print results\n",
    "# Print results with 5 decimal places\n",
    "print(f\"Gradient Descent Î¸: {[f'{x:.5f}' for x in theta_gd]}\")\n",
    "print(f\"Closed-Form Î¸: {[f'{x:.5f}' for x in theta_closed_form]}\")\n",
    "print(f\"Gradient Descent MSE: {mse_gd:.5f}\")\n",
    "print(f\"Closed-Form MSE: {mse_closed_form:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3.2: Closed-form solution training complete on the larger sample.\n",
      "Coefficients: ['-0.04394', '0.00016', '-0.03909', '0.00591', '0.00366']\n"
     ]
    }
   ],
   "source": [
    "# Step 3.2: Use the full dataset and closed form solution\n",
    "X_large_sample, X_large_test_sample, y_large_sample, y_large_test_sample = train_test_split(features, data[target_column], test_size=0.2, random_state=42)\n",
    "\n",
    "# Adding a column of ones for the bias term in the large sample\n",
    "X_large_sample_with_bias = np.c_[np.ones(X_large_sample.shape[0]), X_large_sample]\n",
    "X_large_test_sample_with_bias = np.c_[np.ones(X_large_test_sample.shape[0]), X_large_test_sample]\n",
    "\n",
    "# Upgrade the normal equation\n",
    "theta_large_sample = np.linalg.inv(X_large_sample_with_bias.T @ X_large_sample_with_bias) @ X_large_sample_with_bias.T @ y_large_sample\n",
    "theta_large_sample_rounded = np.round(theta_large_sample, 5)\n",
    "\n",
    "print(f\"Step 3.2: Closed-form solution training complete on the larger sample.\")\n",
    "print(f\"Coefficients: {[f'{x:.5f}' for x in theta_large_sample_rounded]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3.3: Model evaluation on the testing dataset:\n",
      "Root Mean Squared Error (RMSE): 0.03016\n",
      "Mean Squared Error (MSE): 0.00091\n",
      "Mean Absolute Error (MAE): 0.02294\n",
      "R-squared: 0.64752\n"
     ]
    }
   ],
   "source": [
    "# Step 3.3: Verify your model using the testing dataset and appropriate evaluation metrics\n",
    "y_large_pred_closed_form = X_large_test_sample_with_bias @ theta_large_sample\n",
    "\n",
    "mse = mean_squared_error(y_large_test_sample, y_large_pred_closed_form)\n",
    "mae = mean_absolute_error(y_large_test_sample, y_large_pred_closed_form)\n",
    "r2 = r2_score(y_large_test_sample, y_large_pred_closed_form)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Step 3.3: Model evaluation on the testing dataset:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.5f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.5f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.5f}\")\n",
    "print(f\"R-squared: {r2:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Non-linear Regression\n",
    "\n",
    "In Step 1's formulation, if the price $\\lambda$ is treated as a constant and the actual value p is known, the entire formula simplifies into a function dependent on the predicted value $\\hat{p}_t$. This implies that the problem can be reframed as an optimization task concerning the prediction of $\\hat{p}_t$. Given this perspective, extending the linear regression model from Step 3 by incorporating nonlinear features to predict $\\hat{p}_t$ effectively transforms the problem into a nonlinear regression for Step 1's objective. Therefore, performing nonlinear regression on the prediction model of $\\hat{p}_t$ inherently satisfies the requirements of the nonlinear extension outlined in Step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.1: Add polynomial features (squared and cubic terms)\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False) \n",
    "X_poly = poly.fit_transform(features)\n",
    "\n",
    "# Convert to DataFrame with correct column names\n",
    "X_poly_df = pd.DataFrame(X_poly, columns=poly.get_feature_names_out(features.columns))\n",
    "\n",
    "# Split the polynomial feature data\n",
    "X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(X_poly_df, data[target_column], test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit linear regression model on the polynomial features\n",
    "linear_model_poly = LinearRegression()\n",
    "linear_model_poly.fit(X_train_poly, y_train_poly)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred_poly = linear_model_poly.predict(X_test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonlinear model evaluation on the testing dataset:\n",
      "Root Mean Squared Error (RMSE): 0.0278\n",
      "Mean Squared Error (MSE): 0.0008\n",
      "Mean Absolute Error (MAE): 0.0204\n",
      "R-squared: 0.7015\n"
     ]
    }
   ],
   "source": [
    "# evaluate the performance\n",
    "mse_poly = mean_squared_error(y_test_poly, y_pred_poly)\n",
    "mae_poly = mean_absolute_error(y_test_poly, y_pred_poly)\n",
    "r2_poly = r2_score(y_test_poly, y_pred_poly)\n",
    "rmse_poly = np.sqrt(mse_poly)\n",
    "\n",
    "print(f\"Nonlinear model evaluation on the testing dataset:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_poly:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_poly:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_poly:.4f}\")\n",
    "print(f\"R-squared: {r2_poly:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For step 4.2, the method of locally weighted least squares will be used, as tought in the lecture. Different kernels will be compared and the best one will be chosen based on evaluating the performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def gaussian(t):\n",
    "    return np.exp(-0.5 * t**2) / np.sqrt(2 * np.pi)\n",
    "\n",
    "def epanechnikov(t):\n",
    "    res = np.zeros_like(t)\n",
    "    res[np.abs(t) <= 1] = 0.75 * (1 - t[np.abs(t) <= 1]**2)\n",
    "    return res\n",
    "\n",
    "def tricube(t):\n",
    "    res = np.zeros_like(t)\n",
    "    res[np.abs(t) <= 1] = (70 / 81) * (1 - np.abs(t[np.abs(t) <= 1])**3)**3\n",
    "    return res\n",
    "\n",
    "def uniform(t, p=0.2):\n",
    "    return np.zeros_like(t) + p\n",
    "\n",
    "def triangle(t):\n",
    "    res = np.zeros_like(t)\n",
    "    res[np.abs(t) <= 1] = 1 - np.abs(t[np.abs(t) <= 1])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:49:54.327727Z",
     "start_time": "2024-10-07T14:49:54.319484Z"
    }
   },
   "outputs": [],
   "source": [
    "def wls_gaussian(X, y, training_data, radius=0.2):\n",
    "    %time\n",
    "    y_pred_wls = np.zeros(len(training_data))\n",
    "    for i in range(len(training_data)):\n",
    "        W = np.diagflat(gaussian(np.linalg.norm(X - training_data[i], axis=1) / radius))\n",
    "        theta = np.linalg.solve(X.T @ W @ X, X.T @ W @ y.reshape(-1, 1))\n",
    "        y_pred_wls[i] = training_data[i] @ theta\n",
    "    return y_pred_wls\n",
    "\n",
    "def wls_epanechnikov(X, y, training_data, radius=0.2):\n",
    "    %time\n",
    "    y_pred_wls = np.zeros(len(training_data))\n",
    "    for i in range(len(training_data)):\n",
    "        W = np.diagflat(epanechnikov(np.linalg.norm(X - training_data[i], axis=1) / radius))\n",
    "        theta = np.linalg.solve(X.T @ W @ X, X.T @ W @ y.reshape(-1, 1))\n",
    "        y_pred_wls[i] = training_data[i] @ theta\n",
    "    return y_pred_wls\n",
    "\n",
    "def wls_tricube(X, y, training_data, radius=0.2):\n",
    "    %time\n",
    "    y_pred_wls = np.zeros(len(training_data))\n",
    "    for i in range(len(training_data)):\n",
    "        W = np.diagflat(tricube(np.linalg.norm(X - training_data[i], axis=1) / radius))\n",
    "        theta = np.linalg.solve(X.T @ W @ X, X.T @ W @ y.reshape(-1, 1))\n",
    "        y_pred_wls[i] = training_data[i] @ theta\n",
    "    return y_pred_wls\n",
    "\n",
    "def wls_uniform(X, y, training_data, radius=0.2):\n",
    "    %time\n",
    "    y_pred_wls = np.zeros(len(training_data))\n",
    "    for i in range(len(training_data)):\n",
    "        W = np.diagflat(uniform(np.linalg.norm(X - training_data[i], axis=1) / radius))\n",
    "        theta = np.linalg.solve(X.T @ W @ X, X.T @ W @ y.reshape(-1, 1))\n",
    "        y_pred_wls[i] = training_data[i] @ theta\n",
    "    return y_pred_wls\n",
    "\n",
    "def wls_triangle(X, y, training_data, radius=0.2):\n",
    "    %time\n",
    "    y_pred_wls = np.zeros(len(training_data))\n",
    "    for i in range(len(training_data)):\n",
    "        W = np.diagflat(triangle(np.linalg.norm(X - training_data[i], axis=1) / radius))\n",
    "        theta = np.linalg.solve(X.T @ W @ X, X.T @ W @ y.reshape(-1, 1))\n",
    "        y_pred_wls[i] = training_data[i] @ theta\n",
    "    return y_pred_wls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongz\\AppData\\Local\\Temp\\ipykernel_31436\\3043441563.py:7: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  y_pred_wls[i] = training_data[i] @ theta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongz\\AppData\\Local\\Temp\\ipykernel_31436\\3043441563.py:16: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  y_pred_wls[i] = training_data[i] @ theta\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLinAlgError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[95], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Step 4.2 Locally weighted least squared\u001B[39;00m\n\u001B[0;32m      4\u001B[0m y_pred_wls_gaussian \u001B[38;5;241m=\u001B[39m wls_gaussian(X_large_sample_with_bias, y_large_sample, X_large_sample_with_bias)\n\u001B[1;32m----> 5\u001B[0m y_pred_wls_epanechnikov \u001B[38;5;241m=\u001B[39m wls_epanechnikov(X_large_sample_with_bias, y_large_sample, X_large_sample_with_bias)\n\u001B[0;32m      6\u001B[0m y_pred_wls_tricube \u001B[38;5;241m=\u001B[39m wls_tricube(X_large_sample_with_bias, y_large_sample, X_large_sample_with_bias)\n\u001B[0;32m      7\u001B[0m y_pred_wls_uniform \u001B[38;5;241m=\u001B[39m wls_uniform(X_large_sample_with_bias, y_large_sample, X_large_sample_with_bias)\n",
      "Cell \u001B[1;32mIn[92], line 15\u001B[0m, in \u001B[0;36mwls_epanechnikov\u001B[1;34m(X, y, training_data, radius)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(training_data)):\n\u001B[0;32m     14\u001B[0m     W \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdiagflat(epanechnikov(np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(X \u001B[38;5;241m-\u001B[39m training_data[i], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m/\u001B[39m radius))\n\u001B[1;32m---> 15\u001B[0m     theta \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39msolve(X\u001B[38;5;241m.\u001B[39mT \u001B[38;5;241m@\u001B[39m W \u001B[38;5;241m@\u001B[39m X, X\u001B[38;5;241m.\u001B[39mT \u001B[38;5;241m@\u001B[39m W \u001B[38;5;241m@\u001B[39m y\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m     16\u001B[0m     y_pred_wls[i] \u001B[38;5;241m=\u001B[39m training_data[i] \u001B[38;5;241m@\u001B[39m theta\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m y_pred_wls\n",
      "File \u001B[1;32mc:\\Users\\dongz\\anaconda3\\Lib\\site-packages\\numpy\\linalg\\linalg.py:409\u001B[0m, in \u001B[0;36msolve\u001B[1;34m(a, b)\u001B[0m\n\u001B[0;32m    407\u001B[0m signature \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDD->D\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m isComplexType(t) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdd->d\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    408\u001B[0m extobj \u001B[38;5;241m=\u001B[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001B[1;32m--> 409\u001B[0m r \u001B[38;5;241m=\u001B[39m gufunc(a, b, signature\u001B[38;5;241m=\u001B[39msignature, extobj\u001B[38;5;241m=\u001B[39mextobj)\n\u001B[0;32m    411\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrap(r\u001B[38;5;241m.\u001B[39mastype(result_t, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m))\n",
      "File \u001B[1;32mc:\\Users\\dongz\\anaconda3\\Lib\\site-packages\\numpy\\linalg\\linalg.py:112\u001B[0m, in \u001B[0;36m_raise_linalgerror_singular\u001B[1;34m(err, flag)\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_raise_linalgerror_singular\u001B[39m(err, flag):\n\u001B[1;32m--> 112\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m LinAlgError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSingular matrix\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mLinAlgError\u001B[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "y_large_sample = np.array(y_large_sample).reshape(-1, 1)\n",
    "\n",
    "# Step 4.2 Locally weighted least squared\n",
    "y_pred_wls_gaussian = wls_gaussian(X_large_sample_with_bias, y_large_sample, X_large_sample_with_bias)\n",
    "y_pred_wls_epanechnikov = wls_epanechnikov(X_large_sample_with_bias, y_large_sample, X_large_sample_with_bias)\n",
    "y_pred_wls_tricube = wls_tricube(X_large_sample_with_bias, y_large_sample, X_large_sample_with_bias)\n",
    "y_pred_wls_uniform = wls_uniform(X_large_sample_with_bias, y_large_sample, X_large_sample_with_bias)\n",
    "y_pred_wls_triangle = wls_triangle(X_large_sample_with_bias, y_large_sample, X_large_sample_with_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_wls_g = mean_squared_error(y_large_test_sample, y_pred_wls_gaussian)\n",
    "mae_wls_g = mean_absolute_error(y_large_test_sample, y_pred_wls_gaussian)\n",
    "r2_wls_g = r2_score(y_large_test_sample, y_pred_wls_gaussian)\n",
    "rmse_wls_g = np.sqrt(mse_wls_g)\n",
    "\n",
    "print(f\"Weighted Least Squares model evaluation on the testing dataset with Gaussian kernel:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_wls_g:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_wls_g:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_wls_g:.4f}\")\n",
    "print(f\"R-squared: {r2_wls_g:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:59:04.020518Z",
     "start_time": "2024-10-07T14:59:03.903517Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_large_test_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m mse_wls \u001B[38;5;241m=\u001B[39m mean_squared_error(y_large_test_sample,y_pred_wls_epanechnikov)\n\u001B[0;32m      2\u001B[0m mae_wls \u001B[38;5;241m=\u001B[39m mean_absolute_error(y_large_test_sample, y_pred_wls_epanechnikov)\n\u001B[0;32m      3\u001B[0m r2_wls \u001B[38;5;241m=\u001B[39m r2_score(y_large_test_sample, y_pred_wls_epanechnikov)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'y_large_test_sample' is not defined"
     ]
    }
   ],
   "source": [
    "mse_wls_e = mean_squared_error(y_large_test_sample,y_pred_wls_epanechnikov)\n",
    "mae_wls_e = mean_absolute_error(y_large_test_sample, y_pred_wls_epanechnikov)\n",
    "r2_wls_e = r2_score(y_large_test_sample, y_pred_wls_epanechnikov)\n",
    "rmse_wls_e = np.sqrt(mse_wls_e)\n",
    "\n",
    "print(f\"Weighted Least Squares model evaluation on the testing dataset with Epanechnikov kernel:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_wls_e:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_wls_e:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_wls_e:.4f}\")\n",
    "print(f\"R-squared: {r2_wls_e:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_wls_tricube = mean_squared_error(y_large_test_sample, y_pred_wls_tricube)\n",
    "mae_wls_tricube = mean_absolute_error(y_large_test_sample, y_pred_wls_tricube)\n",
    "r2_wls_tricube = r2_score(y_large_test_sample, y_pred_wls_tricube)\n",
    "rmse_wls_tricube = np.sqrt(mse_wls_tricube)\n",
    "\n",
    "print(f\"Weighted Least Squares model evaluation on the testing dataset with tricube kernel:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_wls_tricube:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_wls_tricube:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_wls_tricube:.4f}\")\n",
    "print(f\"R-squared: {r2_wls_tricube:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_wls_uni = mean_squared_error(y_large_test_sample, y_pred_wls_uniform)\n",
    "mae_wls_uni = mean_absolute_error(y_large_test_sample, y_pred_wls_uniform)\n",
    "r2_wls_uni = r2_score(y_large_test_sample, y_pred_wls_uniform)\n",
    "rmse_wls_uni = np.sqrt(mse_wls_uni)\n",
    "\n",
    "print(f\"Weighted Least Squares model evaluation on the testing dataset with uniform kernel:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_wls_uni:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_wls_uni:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_wls_uni:.4f}\")\n",
    "print(f\"R-squared: {r2_wls_uni:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_wls_tri = mean_squared_error(y_large_test_sample, y_pred_wls_triangle)\n",
    "mae_wls_tri = mean_absolute_error(y_large_test_sample, y_pred_wls_triangle)\n",
    "r2_wls_tri = r2_score(y_large_test_sample, y_pred_wls_triangle)\n",
    "rmse_wls_tri = np.sqrt(mse_wls_tri)\n",
    "\n",
    "print(f\"Weighted Least Squares model evaluation on the testing dataset with triangle kernel:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_wls_tri:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_wls_tri:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_wls_tri:.4f}\")\n",
    "print(f\"R-squared: {r2_wls_tri:.4f}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 5 Regularization\n",
    "#### Lasso regularization\n",
    "##### linear regression\n",
    "In order to apply lasso regression there are two prerequisites.  the linear regression has to be performed and the Gradient Descent has to be calculated. \n",
    "The gradient descent is used to update the coefficients iteratively through training instances "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T08:57:34.540820Z",
     "start_time": "2024-10-09T08:57:34.506942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LassoRegression():\n",
    "    def __init__(self, learning_rate, iterations, l1_penalty):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.l1_penalty = l1_penalty\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        self.m, self.n = X.shape\n",
    "        # set the original weights\n",
    "        self.W = np.zeros(self.n)\n",
    "        # set the bias term\n",
    "        self.b = 0 \n",
    "        # X matrix\n",
    "        self.X = X\n",
    "        # Y vector\n",
    "        self.Y = Y\n",
    "        for i in range(self.iterations):\n",
    "            self.update_weights()\n",
    "        return self\n",
    "    \n",
    "    def update_weights(self):\n",
    "        Y_pred = self.predict(self.X)\n",
    "        dW = np.zeros(self.n)\n",
    "        for j in range(self.n):\n",
    "            if self.W[j]>0:\n",
    "                dW[j] = (-2*(self.X[:,j]).dot(self.Y-Y_pred)+self.l1_penalty)/self.m\n",
    "            else:\n",
    "                dW[j] = (-2*(self.X[:,j]).dot(self.Y-Y_pred)-self.l1_penalty)/self.m\n",
    "                \n",
    "        db = -2*np.sum(self.Y-Y_pred)/self.m\n",
    "        \n",
    "        # update weights\n",
    "        self.W = self.W-self.learning_rate*dW\n",
    "        # update bias\n",
    "        self.b = self.b - self.learning_rate*db\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.dot(X, self.W)+self.b \n",
    "            "
   ],
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3345320974.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[1], line 1\u001B[1;36m\u001B[0m\n\u001B[1;33m    class LassoRegression():\u001B[0m\n\u001B[1;37m                            ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m incomplete input\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lasso_model=LassoRegression(learning_rate=0.01, iterations=10000, l1_penalty=0.1)\n",
    "lasso_model.fit(X_large_sample_with_bias,y_large_sample)\n",
    "y_pred_lasso = lasso_model.predict(X_large_test_sample_with_bias)\n",
    "\n",
    "plt.scatter(X_large_test_sample, y_large_sample, color='blue', label='Actual Data')\n",
    "plt.plot(X_large_test_sample_with_bias, y_pred_lasso, color='green', label='Lasso regression')\n",
    "plt.plot(X_large_test_sample_with_bias, y_pred_gd, color='orange', label='Linear regression')\n",
    "plt.title('Linear vs Lasso regression')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Active power production')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
